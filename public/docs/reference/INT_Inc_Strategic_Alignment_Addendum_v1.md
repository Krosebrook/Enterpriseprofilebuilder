# INT Inc. Strategic Alignment Addendum
## Blueprint Enhancement Package | Version 1.0 | December 2025

---

## Document Control

| Attribute | Value |
|-----------|-------|
| **Version** | 1.0 |
| **Date** | December 11, 2025 |
| **Classification** | Internal Strategy + Client-Facing (Appendices) |
| **Parent Document** | INT Inc Complete AI Implementation Blueprint |
| **Purpose** | Resolve narrative contradictions, establish ROI methodology, add SMB framework, create outcomes capture system |

---

## Executive Summary

This addendum addresses three critical gaps identified in the INT Inc Blueprint:

1. **ROI Methodology Appendix** — Reconciles conflicting ROI figures (1.7x vs. 7.4x) with explicit methodology, assumptions, and sensitivity analysis
2. **Failure Mode Countermeasure Framework** — Maps the 70-85% industry failure rate to specific INT Inc interventions with evidence
3. **SMB Implementation Framework** — Provides adjustment factors, realistic expectations, and tailored packages for organizations under 200 employees
4. **Contradiction Resolution Messaging** — Harmonizes "governance-first vs. speed," "augment vs. capacity increase," and "no full automation vs. deflection targets"
5. **Pilot Outcomes Capture System** — Establishes methodology to replace third-party case studies with INT Inc proprietary evidence

**Usage:** This document should be read alongside the main Blueprint. Client-facing appendices (A, B, C) can be extracted for proposals and presentations.

---

# PART 1: ROI METHODOLOGY APPENDIX (IMMEDIATE)

## Appendix A: ROI Projection Methodology & Sensitivity Analysis

*Classification: Client-Facing — Include in proposals and executive presentations*

---

### A.1 The ROI Clarity Problem

The AI industry suffers from inconsistent ROI reporting. Different studies measure different things:
- Some measure "return per dollar invested" (e.g., $1.41/$1)
- Some measure "ROI multiple" (e.g., 1.7x)
- Some measure "productivity value" (e.g., $8,700/employee/year)
- Some measure "cost reduction percentage" (e.g., 21% customer service costs)

**INT Inc's Commitment:** We use a transparent, layered ROI framework that distinguishes between industry benchmarks, initiative-specific projections, and client-actual results. Every projection includes assumptions and sensitivity ranges.

---

### A.2 ROI Metric Taxonomy

| Metric Type | Definition | When to Use | Source Authority |
|-------------|------------|-------------|------------------|
| **Industry Baseline ROI** | Average return across all AI initiatives, all industries, all maturity levels | Setting expectations; "what's normal" | Capgemini, McKinsey, Gartner |
| **Domain-Specific ROI** | Return for specific use cases (e.g., customer service, supply chain) | Tailoring projections to client context | Snowflake/ESG, Axis Intelligence |
| **Initiative Target ROI** | INT Inc's projected return for a specific initiative with stated assumptions | Pilot planning; investment justification | INT Inc methodology |
| **Client-Actual ROI** | Measured return from completed INT Inc engagement | Post-pilot reporting; case studies | INT Inc Pilot Scorecard |

---

### A.3 Validated Industry Benchmarks (2025)

#### A.3.1 General AI ROI

| Metric | Value | Source | Sample Size | Confidence |
|--------|-------|--------|-------------|------------|
| **Average ROI Multiple** | 1.7x | Capgemini June 2025 | n=1,607 orgs | VERY HIGH |
| **Return per $1 Invested** | $1.41 | Snowflake/ESG 2025 | n=1,900 orgs | VERY HIGH |
| **Top 5% Return per $1** | $10.00 | IDC 2024 | Enterprise segment | HIGH |
| **Time to Positive ROI (Leaders)** | 1.8 years | Capgemini 2025 | n=1,607 | VERY HIGH |
| **Time to Positive ROI (Laggards)** | 3.3 years | Capgemini 2025 | n=1,607 | VERY HIGH |

**Interpretation:** The "average" AI initiative returns 1.7x investment, or $1.41 for every $1 invested. This includes successes and failures. Leaders who implement well achieve positive ROI in under 2 years; those who don't may wait over 3 years.

#### A.3.2 Domain-Specific ROI

| Domain | ROI Metric | Value | Source | Confidence |
|--------|------------|-------|--------|------------|
| **Customer Service** | Return per $1 | $3.50 | Industry analysis 2025 | HIGH |
| **Customer Service** | Cost reduction | 21% | Capgemini 2025 | VERY HIGH |
| **Contract Analysis** | ROI | >300% | Capgemini pharma case | HIGH |
| **Supply Chain** | Cost reduction | 23% | Capgemini 2025 | VERY HIGH |
| **Demand Forecasting** | Accuracy improvement | Up to 85% | Capgemini 2025 | HIGH |

**Interpretation:** Customer service AI delivers significantly above-average returns ($3.50 vs. $1.41) due to high volume, measurable deflection, and clear cost attribution.

#### A.3.3 Productivity Benchmarks

| Metric | Value | Source | Context | Confidence |
|--------|-------|--------|---------|------------|
| **Time Savings (Knowledge Workers)** | 7.5–11.4 hrs/week | LSE/Protiviti 2024; Larridin 2025 | Range from multiple studies | HIGH |
| **Annual Productivity Value** | $8,700/employee | Larridin 2025 | Based on 11.4 hrs/week | HIGH |
| **Alternative Calculation** | $18,000/employee | LSE/Protiviti 2024 | Based on 7.5 hrs/week at higher rates | HIGH |
| **Productivity Improvement** | 27% average | Larridin 2025 | Overall task efficiency | HIGH |

**Interpretation:** Conservative estimate: $8,700/employee/year. Optimistic estimate: $18,000/employee/year. INT Inc uses $13,350 midpoint for projections with sensitivity analysis.

---

### A.4 INT Inc Initiative ROI Projections

#### A.4.1 Quick Win Initiatives (Initiatives 1-3)

| Initiative | Investment | Timeline | Target ROI | Assumptions | Sensitivity Range |
|------------|------------|----------|------------|-------------|-------------------|
| **AI Support Copilot** | $4,200 | 6 weeks | 2.5-4.0x | 50-75% deflection, 1,000+ tickets/month, $15-25 cost/ticket | 1.5x (poor KB) to 6.0x (high volume) |
| **Invoice Processing** | $8,500 | 8 weeks | 3.0-5.0x | 500+ invoices/month, current manual processing, clean data | 2.0x (integration issues) to 8.0x (high volume) |
| **Tech Doc Generation** | $2,800 | 8 weeks | 2.0-3.5x | 10+ documents/month, standardizable formats | 1.2x (low volume) to 5.0x (high standardization) |

**Methodology for Quick Wins:**
```
ROI = (Annual Value Created - Total Investment) / Total Investment

Annual Value Created = 
  (Tickets Deflected × Cost per Ticket) + 
  (Hours Saved × Hourly Rate) + 
  (Quality Improvement Value)

Total Investment = 
  INT Inc Fees + 
  Client Staff Time + 
  Technology Costs + 
  Change Management
```

#### A.4.2 Medium Win Initiatives (Initiatives 4-5)

| Initiative | Investment | Timeline | Target ROI | Assumptions | Sensitivity Range |
|------------|------------|----------|------------|-------------|-------------------|
| **QA Test Automation** | $12,500 | 12 weeks | 2.5-4.5x | Current manual QA, 75-85% automation achievable, existing test cases | 1.8x (complex integrations) to 6.0x (high test volume) |
| **Churn Prediction** | $7,200 | 12 weeks | 3.0-6.0x | Sufficient historical data, 260% winback uplift baseline | 1.5x (poor data) to 8.0x (high-value customers) |

**Methodology for Medium Wins:**
```
ROI = NPV(Benefits over 24 months) / Total Investment

Benefits include:
- Direct cost savings (labor, error reduction)
- Revenue protection (churn prevention)
- Capacity increase (handling growth without hiring)
- Quality improvement (reduced rework, higher CSAT)
```

#### A.4.3 Long-Term Platform Initiatives (Initiatives 6-8)

| Initiative | Investment | Timeline | Target ROI | Assumptions | Sensitivity Range |
|------------|------------|----------|------------|-------------|-------------------|
| **Multi-Agent Orchestrator** | $180-250K | 24 months | 5.0-7.4x | Full adoption (85%+), 18-month compounding, multi-agent efficiency, 3+ paying customers | 2.5x (50% adoption) to 10.0x (platform revenue) |
| **CS AI Agent** | $20-25K | 12 months | 2.5-4.0x | 30-40% CSM workload reduction, mature CRM data | 1.5x (integration complexity) to 5.5x (high CSM cost) |
| **Resource Allocation AI** | $18-20K | 12 months | 2.0-3.5x | 20-25% capacity optimization, accurate historical data | 1.2x (data gaps) to 4.5x (high utilization pressure) |

**Methodology for Long-Term Platforms:**
```
ROI = (3-Year NPV of Benefits - Total 3-Year Investment) / Total 3-Year Investment

Benefits include:
- Internal operational savings (Year 1-3)
- External revenue from productized offering (Year 2-3)
- Competitive differentiation value (qualitative)
- Platform reusability across clients (multiplier effect)

Discount Rate: 10% for NPV calculations
```

---

### A.5 The 7.4x Orchestrator Projection: Full Transparency

The Multi-Agent Orchestrator (Initiative 6) targets 7.4x ROI. This is significantly above the 1.7x industry average. Here's why:

#### A.5.1 Assumption Stack

| Assumption | Value | Basis | Risk Level |
|------------|-------|-------|------------|
| **Adoption Rate** | 85% | Microsoft Copilot Champs model with executive sponsorship | MEDIUM — requires active change management |
| **Compounding Period** | 18 months | Time for multi-agent workflows to mature and compound | LOW — industry-validated timeline |
| **Multi-Agent Efficiency Multiplier** | 1.5x | Moody's 35-agent system, Gelato CrewAI deployment | MEDIUM — depends on orchestration quality |
| **Platform Revenue Potential** | $100K+ ARR by Year 3 | 3+ paying customers at $30-50K/year | HIGH — market validation required |
| **Operational Savings** | 35-50% time reduction | Validated by quick win benchmarks | MEDIUM — depends on use case selection |

#### A.5.2 Sensitivity Analysis

| Scenario | Adoption | Platform Revenue | Efficiency | Resulting ROI |
|----------|----------|------------------|------------|---------------|
| **Pessimistic** | 50% | $0 (internal only) | 1.2x | 2.5x |
| **Conservative** | 70% | $50K ARR | 1.3x | 4.2x |
| **Base Case** | 85% | $100K ARR | 1.5x | 7.4x |
| **Optimistic** | 95% | $200K ARR | 1.8x | 10.2x |

#### A.5.3 Go/No-Go Gates

The 7.4x projection is contingent on hitting these gates:

| Gate | Checkpoint | Criterion | If Not Met |
|------|------------|-----------|------------|
| **Gate 1** | Month 4 | POC demonstrates 25%+ efficiency gain | Pause, reassess architecture |
| **Gate 2** | Month 9 | MVP shows 40%+ efficiency gain with 60% adoption | Reduce scope or extend timeline |
| **Gate 3** | Month 14 | 3 internal workflows in production, 1 external pilot | Pivot to internal-only, adjust ROI target |
| **Gate 4** | Month 20 | $50K+ committed ARR from external customers | Cap at 4.0x ROI expectation |

---

### A.6 ROI Calculation Worksheet

*For client-specific projections*

#### Input Variables

| Variable | Client Value | Industry Benchmark | Notes |
|----------|--------------|-------------------|-------|
| Monthly ticket volume | ________ | 5,000-15,000 | |
| Cost per ticket | $________ | $15-25 | Fully loaded agent cost |
| Current deflection rate | ________% | 5-15% | Pre-AI baseline |
| Target deflection rate | ________% | 30-40% | Post-AI target |
| Agent headcount | ________ | Varies | |
| Average agent hourly rate (loaded) | $________ | $35-55 | Include benefits, overhead |
| Annual turnover rate | ________% | 20-40% | |
| Cost to hire/train new agent | $________ | $5,000-15,000 | |

#### Output Projections

| Metric | Calculation | Projected Value |
|--------|-------------|-----------------|
| **Annual Deflection Savings** | (Target - Current Deflection) × Monthly Volume × 12 × Cost/Ticket | $________ |
| **Annual Efficiency Savings** | Hours Saved/Agent × Hourly Rate × 52 × Agent Count | $________ |
| **Annual Turnover Savings** | Turnover Reduction × Agent Count × Cost to Replace | $________ |
| **Total Annual Value** | Sum of above | $________ |
| **Total Investment (Year 1)** | INT Inc fees + technology + internal effort | $________ |
| **Projected Year 1 ROI** | (Annual Value - Investment) / Investment | ________x |
| **Payback Period** | Investment / (Annual Value / 12) | ________ months |

#### Benchmark Comparison

| Metric | Industry Average | Client Projection | Variance |
|--------|------------------|-------------------|----------|
| ROI per $1 invested | $1.41 (general) / $3.50 (CS) | $________ | ________% |
| Time to positive ROI | 12-18 months | ________ months | ________ |
| Deflection rate | 30-40% | ________% | ________% |

---

### A.7 ROI Reporting Standards

#### A.7.1 What We Will Report

| Metric | Frequency | Source | Format |
|--------|-----------|--------|--------|
| Actual deflection rate | Weekly | Ticketing system | Pilot Scorecard |
| Actual cost savings | Monthly | Finance validation | ROI Tracker |
| CSAT impact | Weekly | Survey data | Pilot Scorecard |
| Agent time savings | Bi-weekly | Time study sampling | Efficiency Report |
| Projected vs. actual variance | Monthly | Comparison analysis | Variance Report |

#### A.7.2 What We Will NOT Report

- **Inflated projections:** We will not claim ROI based on "potential" or "theoretical" value
- **Unverified metrics:** All reported values require data source and methodology
- **Conflated timeframes:** Year 1 ROI ≠ Year 3 ROI; we specify timeframe for every projection
- **Cherry-picked comparisons:** We compare to relevant benchmarks, not outlier successes

#### A.7.3 Client Verification Rights

Clients have the right to:
- Request raw data underlying any ROI claim
- Conduct independent verification of savings calculations
- Challenge assumptions and receive revised projections
- Pause or terminate engagement if projected ROI is not tracking

---

# PART 2: FAILURE MODE COUNTERMEASURE FRAMEWORK (IMMEDIATE)

## Appendix B: Why INT Inc Beats the 70-85% Failure Rate

*Classification: Client-Facing — Use in sales conversations and proposals*

---

### B.1 The Industry Failure Problem

**The Statistic:** 70-85% of GenAI deployments fail to meet expected ROI.

**Sources:**
- Gartner 2024: 30% of GenAI projects abandoned after POC
- McKinsey 2025: Only 6% of organizations achieve "high performer" status
- Industry analysis: 42% of companies discarded AI initiatives in 2024 (up from 17% in 2023)

**The Question Every Buyer Should Ask:** "What makes you different from the 70-85% that fail?"

---

### B.2 Root Cause Analysis: Why AI Projects Fail

| Failure Mode | % of Failures | Description | Detection Signal |
|--------------|---------------|-------------|------------------|
| **Poor Data Quality** | 43% | Data not AI-ready; inconsistent, incomplete, or inaccessible | KB gaps, conflicting sources, manual data cleanup required |
| **Unclear Success Criteria** | 25% | No baseline, vague objectives, moving goalposts | "We'll know success when we see it" |
| **No Executive Sponsorship** | 20% | IT-driven deployment without business alignment | 34% adoption rate vs. 89% with sponsorship |
| **Workflow Misalignment** | 12% | AI layered on broken processes instead of redesigned | AI automates bad process faster |

**Source:** Gartner GenAI Survey 2024; McKinsey State of AI 2025; INT Inc analysis

---

### B.3 INT Inc Countermeasure Framework

#### B.3.1 Countermeasure Matrix

| Failure Mode | INT Inc Countermeasure | Methodology Component | Evidence | Detection Checkpoint |
|--------------|------------------------|----------------------|----------|---------------------|
| **Poor Data Quality (43%)** | Discovery Questionnaire Section 3: Data & Knowledge Base Audit | Mandatory KB quality assessment before pilot approval | AssemblyAI: Resolution rates improved 27%→50% only after KB maturation | Day 5 of 30-Day Plan: KB Quality Gate |
| **Unclear Success Criteria (25%)** | Pilot Scorecard with Week 1 Baseline + Quantitative Thresholds | Pre-defined KPIs with specific targets before implementation begins | Go/no-go uses objective metrics, not subjective judgment | Day 1 of 30-Day Plan: Success Criteria Lock |
| **No Executive Sponsorship (20%)** | Champion Program Tier 1 with 10-15% Executive Time Allocation | Weekly Scorecard shared directly with named sponsor | Microsoft model: 89% vs 34% adoption delta | Day 1 of 30-Day Plan: Sponsor Confirmation |
| **Workflow Misalignment (12%)** | 4-Agent Architecture with Iterative Design | Agent feedback loops built into pilot; workflow mapping before automation | High performers 3x more likely to redesign workflows (McKinsey) | Days 6-10 of 30-Day Plan: Workflow Validation |

#### B.3.2 Countermeasure Details

---

**COUNTERMEASURE 1: Data Quality Gate**

*Addresses 43% of failures*

**The Problem:**
"You cannot have good AI with bad data." Organizations rush to deploy AI without assessing whether their knowledge base, training data, or source systems are AI-ready. The result: AI confidently delivers wrong answers, eroding trust and adoption.

**INT Inc Intervention:**

| Phase | Activity | Deliverable | Quality Gate |
|-------|----------|-------------|--------------|
| **Discovery Day 2** | KB inventory and completeness audit | KB Coverage Report | Minimum 70% coverage of pilot ticket types |
| **Discovery Day 3** | KB quality assessment (accuracy, currency, consistency) | KB Quality Score | Minimum 3.5/5.0 quality score |
| **Discovery Day 5** | KB improvement plan with prioritized fixes | KB Remediation Roadmap | Executive sign-off before pilot |
| **Design Week 2** | KB fix sprint for critical gaps | Updated KB Articles | Re-score quality; confirm ≥3.5 |

**Evidence:**
- AssemblyAI case: AI resolution rates started at 27%, improved to 50% only after knowledge base maturation
- Our methodology: We don't skip this step; we gate pilot approval on KB quality

**Client Commitment Required:**
- Allocate KB owner with 20% time during Discovery/Design
- Commit to 2-week KB remediation sprint if quality score <3.5
- Accept delayed pilot launch if data quality is insufficient

---

**COUNTERMEASURE 2: Success Criteria Lock**

*Addresses 25% of failures*

**The Problem:**
Projects fail when success is undefined or constantly redefined. "We'll know it when we see it" is not a success criterion. Without baselines, improvement cannot be measured. Without thresholds, go/no-go decisions become political.

**INT Inc Intervention:**

| Phase | Activity | Deliverable | Quality Gate |
|-------|----------|-------------|--------------|
| **Discovery Day 1** | Define success criteria with quantitative thresholds | Success Criteria Document | Signed by sponsor before Day 2 |
| **Discovery Day 2** | Baseline current performance (4-week lookback) | Baseline Report | Validated by client operations |
| **Pilot Week 6** | Evaluate against pre-defined criteria | Go/No-Go Analysis | Objective comparison: actual vs. target |

**Success Criteria Template:**

| Metric | Current Baseline | Target | Threshold for Success | Measurement Method |
|--------|------------------|--------|----------------------|-------------------|
| CSAT Score | _____ | _____ | Improvement ≥ ____% | Survey (weekly) |
| First Response Time | _____ hrs | _____ hrs | Reduction ≥ ____% | Ticketing system |
| Resolution Time | _____ hrs | _____ hrs | Reduction ≥ ____% | Ticketing system |
| Deflection Rate | _____% | _____% | Achieve ≥ _____% | AI analytics |
| Quality Score | N/A | ≥4.0 | Maintain ≥ 4.0 | Evaluation Scorecard |

**Evidence:**
- Industry pilots with pre-defined success criteria achieve 2.3x higher success rates (McKinsey 2024)
- Our methodology: Success criteria are locked before implementation begins; no mid-flight changes without formal change control

**Client Commitment Required:**
- Sign Success Criteria Document on Day 1
- Provide access to baseline data sources
- Accept that targets, once set, require change control to modify

---

**COUNTERMEASURE 3: Executive Sponsorship Protocol**

*Addresses 20% of failures*

**The Problem:**
IT-driven AI deployments achieve 34% adoption. Executive-sponsored deployments achieve 89% adoption. The delta is 55 percentage points—the difference between success and failure. AI without business alignment becomes a technology science project.

**INT Inc Intervention:**

| Phase | Activity | Deliverable | Quality Gate |
|-------|----------|-------------|--------------|
| **Pre-Engagement** | Identify executive sponsor with decision authority | Named Sponsor Letter | No contract signed without named sponsor |
| **Discovery Day 1** | Sponsor kickoff participation (90 minutes) | Sponsor Alignment Memo | Sponsor confirms objectives, success criteria |
| **Pilot Weekly** | Scorecard shared directly to sponsor | Weekly Scorecard | Sponsor acknowledges receipt (read receipt or reply) |
| **Pilot Week 6** | Go/no-go meeting with sponsor decision | Decision Document | Sponsor signs decision |

**Sponsor Time Commitment:**

| Week | Sponsor Time | Activity |
|------|--------------|----------|
| Week 0 | 90 minutes | Kickoff meeting |
| Weeks 1-6 | 15 minutes | Scorecard review |
| Week 6 | 60 minutes | Go/no-go decision meeting |
| **Total** | ~4 hours | Over 6-week pilot |

**Evidence:**
- Microsoft Copilot Champs: Executive visibility drives 89% adoption
- Axis Intelligence 2025: C-suite demonstration is the #1 predictor of adoption success
- Our methodology: Weekly sponsor engagement is mandatory, not optional

**Client Commitment Required:**
- Name a sponsor with budget authority and operational influence
- Sponsor commits to 4+ hours over pilot duration
- Sponsor will visibly champion the initiative to their organization

---

**COUNTERMEASURE 4: Workflow Redesign Requirement**

*Addresses 12% of failures*

**The Problem:**
Organizations layer AI on top of existing broken processes. This automates inefficiency at scale. High performers are 3x more likely to redesign workflows for AI rather than force AI into legacy processes.

**INT Inc Intervention:**

| Phase | Activity | Deliverable | Quality Gate |
|-------|----------|-------------|--------------|
| **Discovery Day 2** | Current-state workflow mapping | Workflow Documentation | All pilot ticket flows documented |
| **Design Days 6-10** | Future-state workflow design with AI touchpoints | Redesigned Workflow | Agent + ops sign-off on new flow |
| **Pilot Week 3** | Workflow validation (does reality match design?) | Validation Report | Iterate if mismatch >20% |
| **Pilot Week 6** | Workflow optimization based on pilot learnings | Optimized Workflow | Documented improvements |

**Workflow Redesign Checklist:**

| Question | If Answer is "No" |
|----------|-------------------|
| Have we removed unnecessary steps before automating? | Redesign required |
| Does the AI touchpoint add value (not just shift work)? | Reconsider automation |
| Is the human handoff clean (no duplicate effort)? | Redesign handoff |
| Can agents easily override/correct AI output? | Add override mechanism |
| Is the audit trail clear (who did what)? | Add logging |

**Evidence:**
- McKinsey 2025: High performers redesign workflows; others just add AI
- Gelato case: AI reduced carrier onboarding from 5 days to 10 minutes by redesigning the process, not just automating the old one
- Our methodology: We map before we automate; we redesign before we implement

**Client Commitment Required:**
- Ops team participates in workflow mapping (4-8 hours during Discovery)
- Authority to change workflow (not just add technology)
- Acceptance that some processes may not be candidates for AI

---

### B.4 Failure Prevention Dashboard

#### B.4.1 Weekly Risk Assessment

| Risk Indicator | Green (Low Risk) | Yellow (Monitor) | Red (Intervention) |
|----------------|------------------|------------------|-------------------|
| **KB Quality Score** | ≥4.0 | 3.5-3.9 | <3.5 |
| **Sponsor Engagement** | Weekly acknowledgment | Missed 1 week | Missed 2+ weeks |
| **Adoption Rate** | ≥70% | 50-69% | <50% |
| **KPI Progress** | ≥2/3 metrics on track | 1/3 metrics on track | <1/3 metrics on track |
| **Agent Satisfaction** | ≥4.0 | 3.5-3.9 | <3.5 |
| **Escalation Rate** | Within target | 10-20% above target | >20% above target |

#### B.4.2 Intervention Triggers

| Trigger | Condition | Intervention |
|---------|-----------|--------------|
| **Data Quality Alert** | KB score drops below 3.5 | Pause pilot; initiate KB remediation sprint |
| **Sponsor Disengagement** | No scorecard acknowledgment for 2 weeks | Escalate to account executive; schedule sponsor re-engagement |
| **Adoption Stall** | <50% adoption after Week 3 | Emergency champion intervention; identify blockers |
| **CSAT Decline** | CSAT drops >10% from baseline | Pause automation; increase human override; diagnose root cause |
| **Quality Failure** | Quality score <3.5 average or safety incident | Immediate escalation to governance; potential pause |

---

### B.5 Failure Recovery Playbook

*What happens when things go wrong*

#### B.5.1 Scenario: CSAT Drops During Pilot

| Step | Action | Owner | Timeline |
|------|--------|-------|----------|
| 1 | Pause Green Zone automation (revert to human) | Ops Lead | Immediate |
| 2 | Pull sample of low-CSAT tickets for analysis | QA Lead | 24 hours |
| 3 | Identify root cause (AI quality? workflow? expectation?) | Tiger Team | 48 hours |
| 4 | Implement targeted fix (prompt tuning, KB update, workflow change) | Engineering | 3-5 days |
| 5 | Resume automation with increased sampling (20% vs 10%) | Ops Lead | After fix validated |
| 6 | Monitor for 1 week; confirm CSAT recovery | PM | Week 4-5 |

**Success Criterion:** CSAT returns to baseline within 2 weeks of intervention

#### B.5.2 Scenario: Adoption Stalls

| Step | Action | Owner | Timeline |
|------|--------|-------|----------|
| 1 | Survey agents: Why aren't you using the AI? | CX Lead | 24 hours |
| 2 | Categorize blockers: Technical? Training? Trust? Workflow? | Tiger Team | 48 hours |
| 3 | For technical: Fix bugs, improve response time | Engineering | 3-5 days |
| 4 | For training: Emergency training session for low adopters | Champion | 3-5 days |
| 5 | For trust: Showcase wins; peer testimonials; manager reinforcement | Champion + Sponsor | 1 week |
| 6 | For workflow: Redesign AI touchpoint to reduce friction | Ops Lead | 1 week |

**Success Criterion:** Adoption reaches 60%+ within 2 weeks of intervention

#### B.5.3 Scenario: KB Quality Insufficient

| Step | Action | Owner | Timeline |
|------|--------|-------|----------|
| 1 | Flag KB quality issue to sponsor | PM | Immediate |
| 2 | Pause pilot timeline; do not proceed with poor data | PM | Immediate |
| 3 | Scope KB remediation: What articles need update/creation? | KB Owner | 48 hours |
| 4 | Execute KB sprint (prioritize high-volume ticket types) | KB Team | 1-2 weeks |
| 5 | Re-score KB quality | QA Lead | After sprint |
| 6 | Resume pilot only if quality score ≥3.5 | PM | Gate check |

**Success Criterion:** KB quality score ≥3.5 before pilot resumes

---

### B.6 Summary: The INT Inc Difference

| Industry Reality | INT Inc Response |
|------------------|------------------|
| 70-85% of AI projects fail | We systematically address the top 4 failure modes before they occur |
| Most failures are preventable | Our methodology includes detection checkpoints and intervention playbooks |
| Data quality is #1 killer | We gate pilot approval on KB quality; we don't skip this step |
| Executive sponsorship determines adoption | We require named sponsor with defined time commitment |
| AI on broken processes fails | We redesign workflows before we automate them |

**Our Commitment:** We will tell you "not ready" if you're not ready. We would rather delay a pilot than execute a doomed one. Our reputation is built on outcomes, not activity.

---

# PART 3: SMB IMPLEMENTATION FRAMEWORK (SHORT-TERM)

## Appendix C: SMB Adjustment Framework

*Classification: Client-Facing — Use for organizations under 200 employees*

---

### C.1 The SMB Context Challenge

**The Problem:**
Most AI case studies come from enterprise deployments (10,000+ employees). Extrapolating these results to SMBs (under 200 employees) creates unrealistic expectations and implementation failures.

**INT Inc's Reality:**
- Our ICP includes "midsize organizations" (many under 200 employees)
- Enterprise case studies (JPMorgan, Moody's, Toyota) don't directly translate
- SMBs have different constraints: smaller budgets, leaner teams, less IT infrastructure

**Our Commitment:**
This framework provides SMB-specific adjustments, realistic expectations, and tailored packages.

---

### C.2 SMB vs. Enterprise: Key Differences

| Factor | Enterprise (1,000+) | SMB (<200) | Implication |
|--------|---------------------|------------|-------------|
| **Budget** | $100K-500K+ pilot budgets | $15-50K total engagement | Scope reduction required |
| **IT Resources** | Dedicated integration team | Often <1 FTE for all IT | Pre-built integrations essential |
| **Executive Attention** | Dedicated sponsor | Owner wears multiple hats | Async engagement model |
| **Data Maturity** | Data warehouse, BI team | Spreadsheets, fragmented systems | Heavier discovery investment |
| **Change Capacity** | Dedicated change management | "Everyone just figures it out" | Simpler rollout approach |
| **Risk Tolerance** | Structured governance | "Just make it work" | Lighter governance framework |
| **Time Horizon** | 18-24 month ROI acceptable | Need results in 6-12 months | Faster time-to-value focus |

---

### C.3 SMB Adjustment Factors

#### C.3.1 ROI Adjustment

| Metric | Enterprise Benchmark | SMB Adjustment Factor | SMB Expectation | Rationale |
|--------|---------------------|----------------------|-----------------|-----------|
| **Deflection Rate** | 50-75% | 0.7x | 35-52% | Less mature KB, fewer documented processes |
| **Time to Positive ROI** | 12-18 months | 0.8x | 10-14 months | Smaller scope = faster payback |
| **Productivity Savings** | $8,700-18,000/employee | 0.6-0.8x | $5,200-14,400/employee | Lower hourly rates, less billable leverage |
| **Adoption Rate (with sponsorship)** | 89% | 0.9x | 80% | Flatter org = easier communication |
| **CSAT Improvement** | +30-50% | 0.8x | +24-40% | Baseline often higher in SMB (closer to customer) |

**Example Application:**

Enterprise benchmark: 60% deflection rate achievable
SMB adjustment: 60% × 0.7 = 42% deflection rate target

---

#### C.3.2 Timeline Adjustment

| Phase | Enterprise Duration | SMB Duration | SMB Rationale |
|-------|--------------------|--------------| --------------|
| **Discovery** | 2-4 weeks | 1-2 weeks | Smaller scope, fewer stakeholders |
| **Design** | 3-6 weeks | 2-3 weeks | Simpler architecture, standard integrations |
| **Pilot** | 6-12 weeks | 4-8 weeks | Faster iteration cycles |
| **Rollout** | 4-8 weeks | 2-4 weeks | Fewer users to onboard |
| **Total** | 15-30 weeks | 9-17 weeks | ~40% compression |

---

#### C.3.3 Investment Adjustment

| Initiative | Enterprise Investment | SMB Investment | SMB Scope Adjustment |
|------------|----------------------|----------------|----------------------|
| **AI Support Copilot** | $4,200 | $2,500-3,500 | Single channel only (email OR chat) |
| **Invoice Processing** | $8,500 | $4,000-6,000 | Standard format only; no custom integrations |
| **Tech Doc Generation** | $2,800 | $1,500-2,500 | 3-5 template types (not unlimited) |
| **90-Day Pilot** | $15-25K total | $8-15K total | Quick Wins only; Medium Wins as Phase 2 |

---

### C.4 SMB Package: "AI Quick Start"

*Designed specifically for organizations under 200 employees*

#### C.4.1 Package Overview

| Attribute | Specification |
|-----------|---------------|
| **Package Name** | AI Quick Start for SMB |
| **Target Client** | Organizations with 25-200 employees |
| **Investment** | $8,000-15,000 (Phase 1) |
| **Duration** | 6-10 weeks |
| **Scope** | 1 Quick Win initiative |
| **Deliverables** | Pilot outcomes + reusable IP |

#### C.4.2 What's Included

**Discovery (Week 1):**
- 90-minute kickoff with owner/executive
- Data readiness assessment (simplified)
- Success criteria definition
- Workflow mapping (1 process)

**Design (Week 2):**
- Architecture recommendation (pre-built options)
- Risk-tiering (simplified: 2 zones, not 3)
- Integration requirements

**Pilot (Weeks 3-6):**
- Implementation support (2 hours/week)
- Weekly scorecard (simplified)
- Agent training materials

**Outcomes (Weeks 7-8):**
- Results report with validated ROI
- Recommendations for Phase 2
- Reusable templates for internal use

#### C.4.3 What's NOT Included (Phase 2 Options)

| Capability | Why Excluded | Phase 2 Option |
|------------|--------------|----------------|
| Custom integrations | Budget constraint | Add $3-5K for custom API work |
| Multi-channel deployment | Scope constraint | Add $2-3K per additional channel |
| Advanced analytics | Complexity constraint | Add $2-4K for custom dashboards |
| NIST AI RMF compliance | Overhead constraint | Add $3-5K for formal compliance documentation |

#### C.4.4 SMB Success Criteria (Adjusted)

| Metric | SMB Target | Enterprise Comparison |
|--------|------------|----------------------|
| Deflection Rate | 30-45% | 50-75% |
| CSAT Improvement | ≥15% | ≥30% |
| First Response Time Reduction | ≥30% | ≥50% |
| Quality Score | ≥3.8 | ≥4.0 |
| Time to Positive ROI | ≤12 months | ≤18 months |

---

### C.5 SMB Case Study Validation

#### C.5.1 Most Relevant Case Study: H&H Purchasing

| Attribute | Value | SMB Relevance |
|-----------|-------|---------------|
| **Company Type** | Small purchasing firm | Directly comparable |
| **Initiative** | AI-powered invoice processing | Common SMB pain point |
| **Investment** | Not disclosed (estimated $5-10K) | SMB budget range |
| **Results** | 6x capacity increase, 90% cost reduction, $85K savings in 3-month peak | Validates ROI potential |
| **Timeline** | Quick implementation | Validates speed expectation |
| **Source** | Zenphi case study | Third-party validated |

**SMB Translation:**
"If you process 500+ invoices/month and currently do it manually, you could see similar capacity gains. Your $85K savings would scale based on your volume, but even at 1/10th the volume, you're looking at $8,500+ annual savings—more than covering the investment."

#### C.5.2 SMB-Analogous Case Studies

| Case | Original Scale | SMB Translation | Adjusted Expectation |
|------|----------------|-----------------|---------------------|
| **ClickUp (7-day deployment)** | Unknown employee count | Fast implementation is achievable | 2-3 weeks for SMB (less integration complexity) |
| **YAZIO (80% deflection)** | Consumer app company | High deflection possible with good KB | 50-60% for SMB (apply 0.7x factor) |
| **AssemblyAI (97% FRT reduction)** | Tech company | Dramatic FRT improvement possible | 60-80% reduction for SMB |

#### C.5.3 Gaps in SMB Validation (Transparency)

| Gap | Implication | Mitigation |
|-----|-------------|------------|
| Few documented SMB implementations | Cannot guarantee results | Set conservative expectations; track INT Inc outcomes |
| Enterprise case studies dominate | May not translate directly | Apply adjustment factors; monitor actuals |
| No SMB failure case studies | Don't know SMB-specific failure modes | Extra caution in pilot phase; faster pivot triggers |

---

### C.6 SMB Governance: Simplified Framework

*Enterprise governance is too heavy for SMBs. This is the minimum viable governance.*

#### C.6.1 Two-Zone Model (Simplified from Three)

| Zone | Description | AI Authority | Human Role |
|------|-------------|--------------|------------|
| **Green Zone** | Low risk, routine tasks | Full automation | Sampling audit only |
| **Yellow Zone** | Everything else | AI suggests only | Human reviews before send |

*Note: Red Zone (prohibited) is implicit—anything not in Green or Yellow is human-only by default.*

#### C.6.2 SMB Risk Assessment (Simplified)

| Question | If YES | If NO |
|----------|--------|-------|
| Could AI error cost >$500? | Yellow Zone | Consider Green |
| Could AI error cause customer to leave? | Yellow Zone | Consider Green |
| Is this regulated (financial, health, legal)? | Yellow Zone | Consider Green |
| Does this require a promise or commitment? | Yellow Zone | Consider Green |

If all answers are NO → Green Zone
If any answer is YES → Yellow Zone

#### C.6.3 SMB Audit Trail (Minimum)

| What to Log | Where | Retention |
|-------------|-------|-----------|
| AI suggestion made | Ticketing system | 90 days |
| Human accepted/modified/rejected | Ticketing system | 90 days |
| Customer feedback | Survey tool | 1 year |

*Note: Full NIST AI RMF compliance is a Phase 2 add-on for SMBs requiring formal governance documentation.*

---

### C.7 SMB Engagement Model

#### C.7.1 Async-First Communication

| Enterprise Model | SMB Model | Rationale |
|------------------|-----------|-----------|
| Weekly 60-min sponsor meeting | Bi-weekly 30-min check-in | Owner time is scarce |
| Daily standups | Slack/email async updates | No meeting overhead |
| Formal change requests | Quick Slack approval | Move fast |
| Detailed weekly scorecard | Simple 5-metric dashboard | Focus on essentials |

#### C.7.2 Owner Time Commitment (Total)

| Phase | Owner Time | Format |
|-------|------------|--------|
| Kickoff | 90 minutes | Video call |
| Discovery review | 30 minutes | Async (Loom video) |
| Design approval | 30 minutes | Video call |
| Weekly updates | 10 min/week × 4-6 weeks | Email digest |
| Go/no-go | 30 minutes | Video call |
| **Total** | ~4-5 hours | Over 6-10 weeks |

---

# PART 4: CONTRADICTION RESOLUTION MESSAGING (SHORT-TERM)

## Section D: Harmonized Narrative Framework

*Classification: Internal — Use for sales training, proposal writing, and objection handling*

---

### D.1 Contradiction 1: "Governance-First" vs. "Quick Wins in 30 Days"

#### D.1.1 The Tension

**What we say:** "Our core differentiator is governance-first methodology."
**Also what we say:** "Achieve Quick Wins in 30 days."

**Buyer interpretation:** "Governance sounds slow. How can you be governance-first AND fast?"

#### D.1.2 Resolution Framework

**The Truth:** Governance-first ≠ Governance-slow. We front-load governance decisions so implementation can move fast.

**Reframed Messaging:**

> "Governance-first means we make the hard decisions early—not that we move slowly. By Day 5, we've already defined risk tiers, escalation paths, and human oversight requirements. This 'governance infrastructure' then enables rapid, safe automation in Weeks 2-6.
>
> Traditional approaches either skip governance (fast but fragile) or retrofit it later (slow and expensive). We build governance as the accelerant, not the brake.
>
> Think of it like building codes for construction: a building inspector doesn't slow down construction—the inspection happens at defined checkpoints while work continues in parallel. Our governance works the same way."

#### D.1.3 Supporting Evidence

| Day | Governance Activity | Implementation Activity | Parallel Work |
|-----|---------------------|------------------------|---------------|
| Day 1 | Success criteria locked | Kickoff, baseline data pull | ✓ |
| Day 3 | Risk-tiering draft | Workflow mapping | ✓ |
| Day 5 | Risk-tiering approved, escalation paths defined | KB audit complete | ✓ |
| Day 6-10 | Governance checkpoint | Architecture design, prompt development | ✓ |
| Day 11-15 | No new governance (monitoring) | Implementation, shadow mode | ✓ |
| Day 16-25 | Weekly risk review (15 min) | Beta rollout, iteration | ✓ |
| Day 26-30 | Go/no-go governance check | Final adjustments, documentation | ✓ |

**Key Point:** Governance is completed by Day 5. Days 6-30 are implementation with governance monitoring—not governance blocking.

#### D.1.4 Objection Handling Script

**Objection:** "Governance sounds like it will slow things down."

**Response:**
"Actually, governance is what allows us to go fast with confidence. Let me explain: most AI projects stall around Week 4 when someone asks, 'Wait, what if the AI makes a mistake?' and nobody has an answer. By Day 5, we've already answered that question—we've defined what's automated, what requires human review, and what triggers escalation. That's why we can move fast in Weeks 2-4 while others are stuck in meetings debating risk."

---

### D.2 Contradiction 2: "Don't Promise Full Automation" vs. "50-75% Deflection Target"

#### D.2.1 The Tension

**What we say:** "We never promise full automation in early sales cycles."
**Also what we say:** "Target 50-75% ticket deflection."

**Buyer interpretation:** "50-75% deflection sounds like full automation with extra steps."

#### D.2.2 Resolution Framework

**The Truth:** Deflection ≠ Automation. Deflection means "AI drafts the response." Human review can still occur.

**Reframed Messaging:**

> "50-75% deflection means 50-75% of tickets never require human drafting from scratch—but that's not the same as 'no human involvement.'
>
> Our 4-Agent architecture ensures every AI response passes through the Validator before delivery. What changes is the human role: from 'write every response from scratch' to 'review, approve, and occasionally edit AI drafts.'
>
> Agents shift from typists to quality controllers. They spend less time on repetitive writing and more time on complex issues that actually require human judgment.
>
> The deflection rate measures how much low-value typing we eliminate—not how many humans we eliminate."

#### D.2.3 Clarifying Definitions

| Term | Definition | Human Involvement |
|------|------------|-------------------|
| **Deflection** | Ticket resolved without agent drafting response | Validator review (seconds); may include approval click |
| **Full Automation** | Ticket resolved with zero human touchpoints | None—AI handles everything |
| **Human-in-the-Loop** | Agent reviews/edits AI draft before send | 30-60 seconds per ticket |
| **Human-Only** | Agent writes response from scratch | 5-15 minutes per ticket |

**Key Point:** "50% deflection" in our methodology typically means:
- 50% of tickets get AI-drafted responses
- Of those, 80% are approved with <30 seconds human review
- 20% are edited by humans before send
- Effective "zero human touch" is ~40% (50% × 80%), not 50%

#### D.2.4 Objection Handling Script

**Objection:** "50% deflection sounds aggressive. Are you saying AI handles half our tickets with no human involvement?"

**Response:**
"Great question—let me clarify what 'deflection' actually means. When we say 50% deflection, we mean 50% of tickets get AI-drafted responses. But every single response still passes through our Validator agent, and depending on your risk tier, a human may still approve before it goes out. The '50%' measures how much manual drafting work we eliminate, not how many tickets have zero human touchpoints. Your agents become reviewers instead of writers—which is faster, more consistent, and frankly, more satisfying work."

---

### D.3 Contradiction 3: "Augment Not Replace" vs. "6x Capacity Increase"

#### D.3.1 The Tension

**What we say:** "AI augments human agents, it doesn't replace them."
**Also what we say:** "H&H Purchasing achieved 6x capacity increase."

**Buyer interpretation:** "If capacity increases 6x, that means 5 out of 6 people are redundant."

#### D.3.2 Resolution Framework

**The Truth:** Capacity increase ≠ Headcount reduction. Organizations use capacity dividends for growth, quality, and reallocation—not layoffs.

**Reframed Messaging:**

> "6x capacity increase means the same team can handle 6x the workload—but that's rarely what actually happens. In practice, organizations use this capacity dividend in three ways:
>
> 1. **Handle growth without hiring:** As ticket volume increases 20-30% annually, AI absorbs the growth. You don't add headcount; you don't reduce it either.
>
> 2. **Reduce backlog and improve response times:** Same volume, faster resolution, higher CSAT. Customers are happier; agents are less stressed.
>
> 3. **Shift to proactive work:** Agents freed from reactive ticket-grinding can do outreach, training, and relationship-building—work that was always on the 'wish list' but never happened.
>
> We've never seen a client use AI to do a mass layoff. The business case is 'better service at scale,' not 'fewer humans.'"

#### D.3.3 Capacity Reallocation Examples

| Case Study | Capacity Increase | How Capacity Was Used |
|------------|-------------------|----------------------|
| **H&H Purchasing** | 6x | Handled 3x peak season volume without seasonal hiring |
| **ClickUp** | 25% efficiency gain | Freed team for proactive retention vs. reactive support |
| **YAZIO** | Equivalent of 3 FTEs | Reallocated budget to product development |
| **AssemblyAI** | 97% FRT reduction | Agents focused on complex technical support |

#### D.3.4 The Math That Reassures

**Scenario:** 10-person support team, 6x capacity increase

| Option | Outcome | Typical Choice |
|--------|---------|----------------|
| **Mass layoff** | Reduce to 1.7 people | Almost never chosen |
| **Handle growth** | Same 10 people handle 60,000 tickets/month vs. 10,000 | Common in growing orgs |
| **Improve quality** | Same volume, but 50% faster resolution, +30% CSAT | Common in mature orgs |
| **Hybrid** | Handle 2x growth + improve quality + small natural attrition | Most common |

**Key Point:** The question isn't "how many people do we fire?" It's "what do we want our people to do that they can't do today?"

#### D.3.5 Objection Handling Script

**Objection:** "If you're promising 6x capacity, isn't that really about cutting headcount?"

**Response:**
"I understand why you'd ask that, and I want to be direct: we don't pitch AI as a headcount reduction tool. Here's what 6x capacity actually means in practice. [Give H&H example: handled 3x peak without seasonal hiring.] The real question is: what does your team wish they had time to do but can't? Proactive outreach? Better training? Complex problem-solving? That's where the capacity goes. In our experience, organizations use AI to do more with the same team—not the same with fewer people."

---

### D.4 Messaging Consistency Checklist

*Use before any client-facing communication*

| Message | Consistent? | If Not, Fix |
|---------|-------------|-------------|
| ROI projections use documented methodology | ☐ | Reference Appendix A |
| Governance is positioned as accelerant, not brake | ☐ | Use Day 1-5 parallel work framing |
| Deflection is defined (not confused with full automation) | ☐ | Clarify human review still occurs |
| Capacity increase is framed as reallocation, not reduction | ☐ | Provide specific use case examples |
| SMB expectations are adjusted (if applicable) | ☐ | Apply adjustment factors from Appendix C |
| Failure rate statistic includes INT Inc countermeasures | ☐ | Reference Appendix B |

---

# PART 5: PILOT OUTCOMES CAPTURE SYSTEM (ONGOING)

## Section E: INT Inc Proprietary Evidence Development

*Classification: Internal — Methodology for building INT Inc case study library*

---

### E.1 The Evidence Problem

**Current State:**
- INT Inc materials rely on third-party case studies (McKinsey, Gartner, vendor reports)
- Third-party case studies have limitations: enterprise-focused, vendor bias, not INT Inc methodology
- Clients increasingly ask: "What results have YOU achieved?"

**Target State:**
- INT Inc has 10+ proprietary case studies with validated metrics
- Case studies span SMB and enterprise, multiple industries
- Metrics are INT Inc methodology-specific (4-Agent architecture, Risk-Tiering, etc.)

---

### E.2 Outcomes Capture Framework

#### E.2.1 Mandatory Capture Points

| Phase | What to Capture | Who Captures | Where Stored |
|-------|-----------------|--------------|--------------|
| **Discovery** | Baseline metrics (tickets, CSAT, FRT, resolution time) | PM | Client Folder |
| **Discovery** | Client context (industry, size, complexity) | PM | Client Folder |
| **Design** | Architecture decisions and rationale | Tech Lead | Architecture Log |
| **Pilot Week 1** | Initial metrics | PM | Pilot Scorecard |
| **Pilot Weekly** | All scorecard metrics | PM | Pilot Scorecard |
| **Pilot End** | Final results vs. baseline | PM | Results Report |
| **Post-Pilot +30 days** | Sustained results check | Account Manager | Follow-up Report |
| **Post-Pilot +90 days** | Long-term impact | Account Manager | Case Study Draft |

#### E.2.2 Metrics to Track (Mandatory)

| Metric | Baseline | Week 3 | Week 6 | +30 Days | +90 Days |
|--------|----------|--------|--------|----------|----------|
| Deflection Rate | ☐ | ☐ | ☐ | ☐ | ☐ |
| First Response Time | ☐ | ☐ | ☐ | ☐ | ☐ |
| Resolution Time | ☐ | ☐ | ☐ | ☐ | ☐ |
| CSAT Score | ☐ | ☐ | ☐ | ☐ | ☐ |
| Quality Score (avg) | N/A | ☐ | ☐ | ☐ | ☐ |
| Agent Satisfaction | ☐ | N/A | ☐ | ☐ | ☐ |
| Cost per Ticket | ☐ | ☐ | ☐ | ☐ | ☐ |
| Total Investment | N/A | N/A | ☐ | ☐ | ☐ |
| ROI (calculated) | N/A | N/A | ☐ | ☐ | ☐ |

#### E.2.3 Context to Document

| Context Element | Why It Matters | Capture Format |
|-----------------|----------------|----------------|
| **Industry** | Enables industry-specific case studies | Dropdown: Tech, Healthcare, Financial, Professional Services, Retail, Other |
| **Company Size** | Enables SMB vs. Enterprise segmentation | Dropdown: <50, 50-200, 200-1000, 1000+ employees |
| **Ticket Volume** | Enables volume-based benchmarking | Number: Monthly ticket volume |
| **Starting Maturity** | Controls for baseline differences | Score 1-5: KB quality, process maturity, data readiness |
| **INT Inc Scope** | What we actually delivered | Checklist: Discovery, Design, Pilot, Rollout, Managed Service |
| **Unique Challenges** | Rich case study narrative | Free text: Top 3 challenges |
| **Unique Wins** | Rich case study narrative | Free text: Top 3 unexpected wins |

---

### E.3 Case Study Development Process

#### E.3.1 Case Study Qualification Criteria

A pilot qualifies for case study development if:

| Criterion | Threshold | Rationale |
|-----------|-----------|-----------|
| ≥2/3 success criteria met | Required | Can't publish failure case (externally) |
| Client permission obtained | Required | Legal/brand requirement |
| Metrics independently verifiable | Required | Credibility |
| ≥90 days post-pilot | Preferred | Shows sustainability |
| Unique angle available | Preferred | Differentiation from other cases |

#### E.3.2 Case Study Template

**[Company Name] Case Study**

**Executive Summary** (50 words)
[Result headline + key metric]

**The Challenge** (100-150 words)
- Industry context
- Specific pain point
- Why previous approaches failed

**The INT Inc Approach** (150-200 words)
- Discovery findings
- Design decisions (reference 4-Agent architecture, Risk-Tiering)
- Pilot implementation

**The Results** (100-150 words, with metrics table)

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| [Metric 1] | [Value] | [Value] | [%] |
| [Metric 2] | [Value] | [Value] | [%] |
| [Metric 3] | [Value] | [Value] | [%] |

**ROI Summary**

| Investment | Return | ROI | Payback |
|------------|--------|-----|---------|
| $[X] | $[Y]/year | [Z]x | [N] months |

**Client Quote** (1-2 sentences)
"[Attribution to named executive or anonymized role]"

**Key Learnings** (3 bullets)
- [Learning 1]
- [Learning 2]
- [Learning 3]

---

### E.4 Evidence Library Structure

#### E.4.1 Taxonomy

```
INT_Inc_Evidence_Library/
├── Case_Studies/
│   ├── Published/
│   │   ├── CS-001_TechCorp_Support_Copilot.md
│   │   ├── CS-002_HealthCo_Invoice_Processing.md
│   │   └── ...
│   ├── In_Development/
│   │   └── ...
│   └── Declined/
│       └── ...
├── Metrics_Database/
│   ├── All_Pilots_Summary.csv
│   ├── By_Industry/
│   ├── By_Size/
│   └── By_Initiative/
├── Benchmark_Comparisons/
│   ├── INT_vs_Industry.md
│   └── Trend_Analysis.md
└── Templates/
    ├── Case_Study_Template.md
    ├── Metrics_Capture_Sheet.xlsx
    └── Client_Permission_Form.docx
```

#### E.4.2 Metrics Database Schema

| Field | Type | Description |
|-------|------|-------------|
| `pilot_id` | String | Unique identifier |
| `client_name` | String | Client name (internal only) |
| `industry` | Enum | Industry category |
| `employee_count` | Integer | Company size |
| `ticket_volume_monthly` | Integer | Monthly ticket volume |
| `initiative_type` | Enum | Support Copilot, Invoice, Docs, etc. |
| `baseline_deflection` | Float | Pre-pilot deflection rate |
| `final_deflection` | Float | Post-pilot deflection rate |
| `baseline_frt` | Float | Pre-pilot FRT (hours) |
| `final_frt` | Float | Post-pilot FRT (hours) |
| `baseline_csat` | Float | Pre-pilot CSAT |
| `final_csat` | Float | Post-pilot CSAT |
| `total_investment` | Float | Total pilot investment |
| `annual_savings` | Float | Calculated annual savings |
| `roi_multiple` | Float | Calculated ROI |
| `payback_months` | Float | Months to payback |
| `case_study_eligible` | Boolean | Meets publication criteria |
| `case_study_status` | Enum | Not Started, In Development, Published, Declined |

---

### E.5 Evidence Usage Guidelines

#### E.5.1 When to Use INT Inc Evidence vs. Third-Party

| Situation | Use INT Inc Evidence | Use Third-Party Evidence |
|-----------|---------------------|-------------------------|
| "What results have YOU achieved?" | ✓ | |
| Setting expectations for specific initiative | ✓ (if comparable case exists) | ✓ (if INT Inc case doesn't exist) |
| Industry-specific proof points | ✓ (if industry match) | ✓ (if no INT Inc match) |
| ROI methodology validation | | ✓ (Capgemini, McKinsey, etc.) |
| Market sizing / adoption trends | | ✓ (industry research) |

#### E.5.2 Citation Standards

**INT Inc Evidence:**
> "In a recent INT Inc engagement with a [industry] company of [size], we achieved [metric]. [Attribution: INT Inc Pilot Data, [Date]]"

**Third-Party Evidence with INT Inc Validation:**
> "Industry benchmarks show [metric] (Source: [Capgemini/McKinsey/etc.]). In our experience, clients achieve [INT Inc observed range], with [context for variance]."

**Third-Party Evidence Only:**
> "[Metric] (Source: [Full citation]). Note: This is industry benchmark data; actual results vary based on implementation quality and organizational readiness."

---

### E.6 Quarterly Evidence Review

#### E.6.1 Review Cadence

| Activity | Frequency | Owner | Output |
|----------|-----------|-------|--------|
| Metrics database update | Weekly | PM (per engagement) | Updated database |
| Case study pipeline review | Monthly | Marketing Lead | Pipeline status |
| Benchmark comparison update | Quarterly | Strategy Lead | INT Inc vs. Industry report |
| Evidence library audit | Quarterly | Marketing Lead | Completeness check |

#### E.6.2 Quarterly Evidence Report Template

**INT Inc Evidence Report: Q[X] 202[X]**

**Summary Statistics**

| Metric | This Quarter | YTD | All-Time |
|--------|--------------|-----|----------|
| Pilots completed | | | |
| Case studies published | | | |
| Average ROI achieved | | | |
| Average deflection rate | | | |

**Portfolio Performance**

| Initiative Type | # Pilots | Avg ROI | Avg Deflection | Success Rate |
|-----------------|----------|---------|----------------|--------------|
| Support Copilot | | | | |
| Invoice Processing | | | | |
| Tech Docs | | | | |
| Other | | | | |

**INT Inc vs. Industry Benchmark**

| Metric | Industry Avg | INT Inc Avg | Delta |
|--------|--------------|-------------|-------|
| ROI | 1.7x | | |
| Time to positive ROI | 18 months | | |
| Deflection rate | 35% | | |

**Key Findings**
- [Finding 1]
- [Finding 2]
- [Finding 3]

**Actions for Next Quarter**
- [Action 1]
- [Action 2]

---

# PART 6: INTEGRATION GUIDE

## How This Addendum Connects to the Blueprint

---

### F.1 Document Hierarchy

```
INT Inc Complete AI Implementation Blueprint (Parent)
├── Component 1: Validated AI Pilot Framework
│   └── + Appendix B: Failure Mode Countermeasures (NEW)
├── Component 3: WSJF-Prioritized Initiative Portfolio
│   └── + Appendix A: ROI Methodology (NEW)
├── Component 5: AI-Enabled Package Definition Template
│   └── + Appendix C: SMB Adjustment Framework (NEW)
├── Component 10: Failure Mode Playbook
│   └── + Appendix B: Failure Mode Countermeasures (NEW)
└── NEW: Ongoing Evidence Development
    └── + Section E: Pilot Outcomes Capture System (NEW)
```

---

### F.2 Cross-Reference Matrix

| Blueprint Section | Addendum Section | Integration Point |
|-------------------|------------------|-------------------|
| Component 1.2 (ROI Methodology) | Appendix A | Replace/expand ROI benchmarks |
| Component 1.5 (Failure Cases) | Appendix B | Add systematic countermeasures |
| Component 3.1 (Initiative ROI) | Appendix A.4 | Reference methodology for all projections |
| Component 5 (Package Templates) | Appendix C | Add SMB-specific package variant |
| Component 10 (Failure Playbook) | Appendix B.5 | Add intervention protocols |
| Section 5.4 (Sales Enablement) | Section D | Add objection handling scripts |
| All case study references | Section E | Plan for INT Inc evidence replacement |

---

### F.3 Update Cadence

| Addendum Section | Update Frequency | Trigger |
|------------------|------------------|---------|
| Appendix A (ROI) | Quarterly | New industry research (Capgemini, McKinsey) |
| Appendix B (Countermeasures) | Semi-annually | New failure mode identification |
| Appendix C (SMB) | Quarterly | INT Inc SMB pilot completion |
| Section D (Messaging) | As needed | New contradiction identified |
| Section E (Evidence) | Ongoing | Every pilot completion |

---

### F.4 Version Control

| Version | Date | Changes |
|---------|------|---------|
| 1.0 | December 11, 2025 | Initial release: ROI Methodology, Failure Countermeasures, SMB Framework, Contradiction Resolution, Evidence Capture |
| 1.1 | [Planned Q1 2026] | First quarterly refresh with INT Inc pilot data |

---

## Document Approval

| Role | Name | Date | Signature |
|------|------|------|-----------|
| AI Portfolio Lead | | | |
| CFO | | | |
| VP Sales | | | |
| VP Operations | | | |

---

*INT Inc. Strategic Alignment Addendum*
*Version 1.0 | December 11, 2025*
*Classification: Internal Strategy + Client-Facing Appendices*
