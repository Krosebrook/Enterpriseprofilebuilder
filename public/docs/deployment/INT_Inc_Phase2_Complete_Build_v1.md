# INT Inc. Phase 2: Complete Sales & Market Enablement Build-Out
## Maximum Depth Execution Package | Version 1.0 | December 2025

---

## Document Control

| Attribute | Value |
|-----------|-------|
| **Version** | 1.0 |
| **Date** | December 11, 2025 |
| **Classification** | Internal Strategy + Client-Facing Components |
| **Parent Document** | INT Inc Blueprint Enhancement Package v2.0 |
| **Phase** | 2 of 3 — Sales & Market Enablement |
| **Total Effort** | ~16 hours (as estimated in roadmap) |
| **Dependencies** | Phase 1 complete (ROI Methodology, Failure Countermeasures, Contingencies) |

---

## Executive Summary

**TL;DR:** This document builds out Phase 2 of the INT Inc strategic roadmap at maximum depth—covering competitive intelligence, SMB packaging, document architecture, and sales tool integration. Each section includes actionable frameworks, scripts, templates, and operational procedures ready for immediate deployment.

### Phase 2 Components

| Task | Component | Est. Effort | Output Type |
|------|-----------|-------------|-------------|
| 4 | Competitive Intelligence Framework | 4 hrs | Sales enablement + positioning |
| 5 | SMB Fast Track Package | 8 hrs | Product/service package |
| 6 | Document Dependency Map | 2 hrs | Operational tool |
| 7 | Platform Explorer Sales Integration | 2 hrs | Sales process documentation |

### Key Outcomes

Upon completion of Phase 2, INT Inc will have:
1. **Complete competitive playbook** with scripts for every competitor type
2. **Productized SMB offering** with pricing, SOW templates, and adjusted benchmarks
3. **Living document architecture** that prevents version confusion
4. **Integrated sales demo workflow** that uses Platform Explorer as a sales tool

---

# TASK 4: COMPETITIVE INTELLIGENCE FRAMEWORK (MAXIMUM DEPTH)

## 4.1 Strategic Positioning Foundation

### 4.1.1 The Core Positioning Statement

**For executives who believe AI is strategic but struggle to see ROI,** INT Inc provides governance-first AI implementation consulting **that delivers measurable value in weeks, not years.**

**Unlike Big 4 consultancies** that require 6-12 month discoveries and $500K+ budgets, **INT Inc offers 6-week pilots** with senior practitioners who transfer IP and capability to your team.

**The proof:** While 70-85% of AI projects fail to meet ROI expectations, our methodology specifically addresses the failure modes that cause this—resulting in **89% adoption rates** when properly implemented vs. the industry average of 34%.

### 4.1.2 Three-Sentence Value Proposition

Use this in every first conversation:

> "88% of organizations are experimenting with AI, but only 6% are achieving enterprise-wide impact. That 82-point gap isn't a technology problem—it's a methodology problem. We help organizations close that gap through governance-first implementation that delivers board-ready ROI in 6 weeks."

### 4.1.3 The INT Inc Difference (One Slide)

| What Others Do | What INT Inc Does |
|----------------|-------------------|
| Sell AI technology | Sell AI outcomes |
| 6-12 month discovery | 6-week pilots with measurable results |
| Junior consultants learning on your dime | Senior practitioners who transfer knowledge |
| Platform lock-in | Technology-agnostic recommendations |
| Generic frameworks | NIST AI RMF-aligned governance |
| Create consulting dependency | Transfer IP and capability |

---

## 4.2 Competitor Deep Dives

### 4.2.1 Big 4 / Global Consultancies — Complete Analysis

#### Deloitte

**Profile:**
- Global AI services revenue: $3B+ annually
- Typical engagement: $500K-5M, 6-18 months
- Delivery model: Partner-led sales, junior-heavy delivery
- Key partnerships: Microsoft, AWS, Google Cloud, ServiceNow

**Approach:**
- Large transformation programs ("Deloitte AI Institute")
- Heavy on strategy, lighter on hands-on implementation
- Industry-specific AI frameworks (financial services, healthcare, etc.)
- Emphasis on enterprise-scale transformation

**Strengths:**
1. **Global scale:** Can staff 50+ person teams; regulatory expertise across jurisdictions
2. **Brand credibility:** Board-level trust; procurement-approved vendor
3. **Compliance expertise:** Deep regulatory knowledge (SOC, HIPAA, GDPR, industry-specific)
4. **Research arm:** Deloitte Insights produces thought leadership

**Weaknesses:**
1. **Slow:** 3-6 month sales cycles; 3-6 month discovery before pilot
2. **Expensive:** $150-300/hour blended rates; minimum engagements often $250K+
3. **Junior-heavy delivery:** Partners sell, associates deliver
4. **Template-driven:** Generic frameworks; less customization for mid-market
5. **Handoff issues:** Strategy teams hand to implementation teams; context loss

**INT Inc Counter-Strategy:**

*When prospect mentions Deloitte:*

> "Deloitte is excellent for large-scale transformations—but their sweet spot is $500K+ programs with 6-month discoveries. If you have that budget and timeline, they're a solid choice. If you want to see results in the next quarter with $50K, we're built for that. We're not trying to compete with Deloitte—we're solving a different problem: getting to value fast with senior practitioners who actually build, not just advise."

*Discovery questions to reveal Deloitte fit issues:*
1. "What's your timeline for seeing first results?"
2. "Who from [Deloitte] would be doing the day-to-day work?"
3. "Have you seen their proposed team composition? What's the ratio of partners to associates?"
4. "What happens after the 'assessment phase'—do they implement, or hand off?"

**Proof points to deploy:**
- "6-week pilot vs. 6-month discovery"
- "Our senior practitioners have 15+ years experience; they don't send associates"
- "We transfer IP—you own the playbooks when we leave"

---

#### Accenture

**Profile:**
- AI-specific revenue: $2B+ annually (part of larger digital transformation)
- Typical engagement: $200K-2M, platform implementations
- Delivery model: Partner + offshore leverage
- Key partnerships: Microsoft (deep), Salesforce, Adobe, SAP

**Approach:**
- Technology-led; "platform before process"
- Heavy Microsoft partnership (Copilot implementations)
- Offshore delivery centers for implementation scale
- Focus on Fortune 500

**Strengths:**
1. **Technical depth:** Large engineering bench; can build complex integrations
2. **Platform relationships:** Preferred partner for Microsoft, Salesforce; early access to features
3. **Implementation capacity:** Can scale to 100+ person teams
4. **Industry solutions:** Pre-built accelerators for common use cases

**Weaknesses:**
1. **Platform bias:** Will recommend Microsoft/Salesforce regardless of fit
2. **Offshore-heavy:** Quality variability; timezone challenges
3. **Change management gap:** Strong on tech, weaker on adoption
4. **Vendor lock-in:** Solutions often dependent on specific platforms
5. **Overly complex:** Tendency to over-engineer; "enterprise" everything

**INT Inc Counter-Strategy:**

*When prospect mentions Accenture:*

> "Accenture builds excellent platforms—if you need a $500K Microsoft Copilot rollout, they're the partner to call. But here's what we see: organizations implement the platform and then adoption stalls at 34%. The technology works; the people don't use it. We focus on the 70% that isn't technology—data readiness, workflow redesign, change management. We're technology-agnostic because we don't make money selling you licenses."

*Discovery questions to reveal Accenture fit issues:*
1. "How did they assess your data readiness before recommending a platform?"
2. "What's included in change management and adoption support?"
3. "Are you committed to [Microsoft/Salesforce], or still evaluating options?"
4. "What happens if the platform they recommend doesn't deliver expected ROI?"

**Proof points to deploy:**
- "We're platform-agnostic—we help you succeed regardless of technology choice"
- "89% adoption with executive sponsorship vs. 34% without—that's not a technology problem"
- "We design workflows first, then select tools; they often do the opposite"

---

#### EY / PwC / McKinsey (Consolidated)

**Profile:**
- Strategy-heavy; implementation-light
- Typical engagement: $100K-1M+ for strategy; implementation often subcontracted
- Strong C-suite access; board credibility

**Strengths:**
1. **Strategic credibility:** Board presentations; investor-grade deliverables
2. **C-suite access:** Partners have existing relationships
3. **Governance expertise:** Strong risk and compliance frameworks
4. **Research quality:** Data-driven recommendations

**Weaknesses:**
1. **Implementation gap:** Strategy ends where execution begins
2. **Handoff model:** Often hand to systems integrators (Accenture, Wipro) for build
3. **Expensive advice:** $500-1000/hour for senior partners; much of that is thinking, not building
4. **Slow cycle:** Months of assessment before any value delivered

**INT Inc Counter-Strategy:**

> "McKinsey, EY, and PwC produce excellent strategy documents—and then hand off to someone else to build. We've seen those strategy decks sit on shelves for 12 months while competitors ship. Our approach is different: we start with a pilot that proves the strategy works. Strategy emerges from validated results, not the other way around. If you need a strategy deck for the board, they're the right choice. If you need results, talk to us."

---

### 4.2.2 AI Pure-Play Vendors — Complete Analysis

#### Microsoft Copilot / Copilot Studio

**Profile:**
- Pricing: $30/user/month (M365 Copilot); $200/month (Copilot Studio)
- Distribution: 400M+ M365 users; massive install base
- Approach: "AI for everyone"; horizontal productivity

**Strengths:**
1. **Distribution:** Already in most enterprises; no new vendor approval
2. **Integration depth:** Native to M365; Teams, Outlook, Excel, Word
3. **Familiar interface:** Users already in the ecosystem
4. **Enterprise security:** Inherits M365 compliance posture

**Weaknesses:**
1. **Generic:** Not industry-specific; not role-specific
2. **Adoption challenges:** Microsoft reports 34% adoption without change management
3. **ROI measurement:** Hard to attribute savings to Copilot specifically
4. **Customization limits:** Copilot Studio adds flexibility but requires development

**INT Inc Opportunity:**

> "Microsoft Copilot is excellent—and most of our clients already have licenses. The problem isn't the tool; it's getting people to use it. Microsoft's own data shows 34% adoption when IT just turns it on, vs. 89% with executive sponsorship and change management. That's where we come in: we help you extract value from the licenses you're already paying for."

**Service Offering:**
- **Copilot Adoption Accelerator** ($15-25K, 6 weeks)
- Use case identification workshop
- Champion program setup
- Adoption tracking dashboard
- Change management playbook

---

#### Salesforce Einstein / Agentforce

**Profile:**
- Pricing: Included with Salesforce (Einstein); $2/conversation (Agentforce)
- Distribution: 150K+ Salesforce customers
- Approach: CRM-native AI; "AI inside your workflow"

**Strengths:**
1. **CRM context:** Deep access to customer data; no integration needed
2. **Trust layer:** Salesforce Trust handles data residency, compliance
3. **Ecosystem:** Connects to Service Cloud, Marketing Cloud, etc.
4. **Autonomous agents:** Agentforce enables autonomous task completion

**Weaknesses:**
1. **Salesforce lock-in:** Only works within Salesforce ecosystem
2. **Pricing uncertainty:** Agentforce usage-based pricing hard to predict
3. **Data quality dependency:** AI only as good as CRM data
4. **Customization complexity:** Advanced use cases require Apex development

**INT Inc Opportunity:**

> "Salesforce Einstein and Agentforce are powerful—if your CRM data is clean and your workflows are well-defined. What we find is that organizations turn on Einstein and get mediocre results because their underlying data is messy. We help you fix that: data quality assessment, workflow redesign, and then AI activation. The AI is free with your license; getting value from it isn't."

---

#### OpenAI / ChatGPT Enterprise

**Profile:**
- Pricing: $60/user/month (Enterprise); usage-based for API
- Distribution: 200M+ users; 92% of Fortune 500
- Approach: "AI platform"; broad capability

**Strengths:**
1. **Model capability:** GPT-4 is among the best for reasoning, writing, analysis
2. **Brand recognition:** "ChatGPT" is synonymous with AI for many executives
3. **Custom GPTs:** Easy to create role-specific assistants
4. **Enterprise security:** SOC 2 Type 2; no training on customer data

**Weaknesses:**
1. **Generic platform:** Not purpose-built for any specific workflow
2. **Adoption requires behavior change:** Users must remember to use it
3. **Integration gaps:** Not embedded in existing workflows without development
4. **Shadow IT risk:** Free tier creates ungoverned usage

**INT Inc Opportunity:**

> "ChatGPT is powerful for individuals—but scaling it across an organization is a governance challenge. Who's using it? What data are they putting in? How do you ensure consistent quality? We help you move from 'random experimentation' to 'governed deployment' with use case prioritization, policy frameworks, and adoption tracking."

---

### 4.2.3 Boutique Competitors — Complete Analysis

#### Technical Boutiques (Data Science Firms)

**Examples:** Domino Data Lab services, boutique ML consultancies

**Profile:**
- Team size: 5-50 people
- Typical engagement: $50-200K, project-based
- Approach: Technical excellence; custom solutions

**Strengths:**
1. **Technical depth:** Deep ML/AI expertise; PhDs on staff
2. **Customization:** Will build exactly what you need
3. **Speed:** Small teams, fast decisions
4. **Cost:** 30-50% less than Big 4 for similar work

**Weaknesses:**
1. **Governance gaps:** "Move fast and break things" culture
2. **Documentation light:** Code works but isn't documented for auditors
3. **Key person dependency:** One senior person knows everything
4. **Scalability:** Can't staff large programs

**INT Inc Counter-Strategy:**

> "Boutique data science firms are technically excellent—but they're often allergic to documentation and governance. When your auditor asks 'how did you test for bias?', you need more than working code. Our methodology includes governance by design: every model we help deploy has an audit trail, risk assessment, and compliance documentation. We move fast AND leave you audit-ready."

---

#### Fractional AI Leaders

**Examples:** Interim CAOs, fractional CTOs with AI focus

**Profile:**
- Engagement: $15-30K/month, part-time
- Approach: Executive guidance; strategy advisory

**Strengths:**
1. **Executive credibility:** C-level experience
2. **Flexibility:** Part-time, can flex up/down
3. **Network:** Connections to talent, vendors
4. **Speed:** Fast to engage; no procurement hurdles

**Weaknesses:**
1. **Single point of failure:** One person knows everything
2. **Limited implementation capacity:** Advise, don't build
3. **Continuity risk:** What happens when they leave?
4. **Breadth vs. depth:** May lack deep technical chops

**INT Inc Counter-Strategy:**

> "Fractional AI leaders are great for strategic guidance—but what happens when they leave? Our approach is different: we transfer IP. The playbooks, templates, and frameworks we build together belong to you. Your team learns to run the process, not just follow the advice of one person."

---

### 4.2.4 Internal DIY — Complete Analysis

**When Organizations Go DIY:**
- Believe AI is "just technology" that IT can figure out
- Had bad experiences with consultants; don't trust external advice
- Have strong internal technical team; want to build capability
- Budget constraints; can't afford external help

**Why DIY Often Fails:**

| Factor | DIY Challenge | INT Inc Solution |
|--------|---------------|------------------|
| **Learning curve** | 18-24 months to develop AI expertise | Accelerate with 550+ case study patterns |
| **Failure rate** | 70-85% of projects fail | Methodology specifically addresses failure modes |
| **Governance gaps** | IT builds; compliance catches up later | Governance-first from day one |
| **Opportunity cost** | Best engineers on AI experimentation, not core product | External experts handle AI; your team stays focused |
| **Adoption** | "Build it and they won't come" (34% adoption) | Change management embedded in methodology |

**INT Inc Counter-Strategy:**

> "Building internal AI capability is absolutely the right long-term play—the question is how you get there. Option A: spend 18-24 months learning from trial and error, with a 70-85% chance of failure on your first few projects. Option B: work with us on your first pilot, learn our methodology alongside us, and then run pilots two through ten yourself with the playbooks we've transferred. We're not a crutch; we're a shortcut."

**Positioning for DIY situations:**
- **Never say:** "You can't do this without us"
- **Always say:** "We accelerate your capability building"
- **Offer:** "Co-creation" where their team learns alongside ours
- **Proof point:** Starter Kit IP transfer shows we don't create dependency

---

## 4.3 Competitive Battle Cards (One-Page Formats)

### 4.3.1 Battle Card: vs. Big 4

```
┌─────────────────────────────────────────────────────────────────┐
│                    INT INC vs. BIG 4                            │
│                  (Deloitte, Accenture, EY, PwC, McKinsey)       │
├─────────────────────────────────────────────────────────────────┤
│ LEAD WITH:                                                       │
│ ✓ Speed to value: "6-week pilots vs. 6-month discovery"         │
│ ✓ Senior delivery: "Our practitioners average 15+ years"        │
│ ✓ IP transfer: "You own the playbooks when we leave"            │
│ ✓ Right-sized investment: "$15-50K pilots, not $500K programs"  │
├─────────────────────────────────────────────────────────────────┤
│ AVOID:                                                          │
│ ✗ Price competition (we won't beat their scale economics)       │
│ ✗ Global reach claims (they win on geographic coverage)         │
│ ✗ Brand credibility arguments (boards trust Big 4 names)        │
├─────────────────────────────────────────────────────────────────┤
│ KEY QUESTIONS TO ASK:                                           │
│ 1. "Who specifically will be doing the work—partners or junior  │
│     associates?"                                                │
│ 2. "What's your timeline for first measurable results?"         │
│ 3. "After the 'strategy phase,' who does implementation?"       │
│ 4. "What IP do you retain vs. what do they take?"               │
├─────────────────────────────────────────────────────────────────┤
│ OBJECTION HANDLING:                                             │
│                                                                 │
│ "Deloitte has more resources"                                   │
│ → "Resources matter less than methodology. 70% of those         │
│    resources are junior analysts. Our senior team has done      │
│    this hundreds of times."                                     │
│                                                                 │
│ "The board prefers Big 4"                                       │
│ → "Understood. What if we run a pilot alongside their strategy  │
│    work? By the time their assessment is done, we'll have       │
│    proven ROI. Then you have data for the board."               │
├─────────────────────────────────────────────────────────────────┤
│ PROOF POINTS:                                                   │
│ • 6-week pilot timeline (vs. 6-12 month Big 4 discovery)        │
│ • $15-50K entry point (vs. $500K+ Big 4 minimums)               │
│ • INT Inc Starter Kit—client owns all IP post-engagement        │
│ • 89% adoption rate with our methodology vs. 34% industry avg   │
└─────────────────────────────────────────────────────────────────┘
```

### 4.3.2 Battle Card: vs. AI Pure-Plays

```
┌─────────────────────────────────────────────────────────────────┐
│                INT INC vs. AI PURE-PLAYS                        │
│         (Microsoft, Salesforce, OpenAI, C3.ai, DataRobot)       │
├─────────────────────────────────────────────────────────────────┤
│ LEAD WITH:                                                       │
│ ✓ Technology agnosticism: "We help you succeed with ANY tool"   │
│ ✓ The 70% that isn't tech: "Data, workflows, change, governance"│
│ ✓ Adoption expertise: "89% adoption vs. 34% when just deploying"│
│ ✓ No platform lock-in: "We don't sell licenses; we sell ROI"    │
├─────────────────────────────────────────────────────────────────┤
│ AVOID:                                                          │
│ ✗ Technical depth competition (they have better engineering)    │
│ ✗ Platform feature debates (they know their products better)    │
│ ✗ Pricing comparison (their per-seat model is different)        │
├─────────────────────────────────────────────────────────────────┤
│ KEY QUESTIONS TO ASK:                                           │
│ 1. "How did they assess your data readiness before recommending │
│     their platform?"                                            │
│ 2. "What's the change management and adoption plan?"            │
│ 3. "What happens if the tool doesn't deliver expected ROI—are   │
│     you locked in?"                                             │
│ 4. "Who helps you after the platform is deployed?"              │
├─────────────────────────────────────────────────────────────────┤
│ OBJECTION HANDLING:                                             │
│                                                                 │
│ "Microsoft/Salesforce AI is included free"                      │
│ → "Those are excellent tools—and most of our clients have them. │
│    The platform is 30% of success. The other 70%—data quality,  │
│    workflow design, adoption, governance—that's where value     │
│    actually comes from. That's what we do."                     │
│                                                                 │
│ "We're already using ChatGPT/Copilot"                           │
│ → "Great! How's adoption? What's your measured ROI? If you're   │
│    seeing the results you expected, wonderful. If not, we help  │
│    diagnose why. Usually it's not the tool."                    │
├─────────────────────────────────────────────────────────────────┤
│ PROOF POINTS:                                                   │
│ • 89% adoption with methodology vs. 34% with "just deploy"      │
│ • Customer service AI: $3.50 return per $1 (not from platform)  │
│ • NIST AI RMF governance framework (platforms don't provide)    │
│ • Failure mode countermeasures addressing 70-85% failure rate   │
└─────────────────────────────────────────────────────────────────┘
```

### 4.3.3 Battle Card: vs. Boutiques

```
┌─────────────────────────────────────────────────────────────────┐
│                  INT INC vs. BOUTIQUES                          │
│         (Data science firms, fractional leaders, shops)         │
├─────────────────────────────────────────────────────────────────┤
│ LEAD WITH:                                                       │
│ ✓ Governance by design: "Auditable when the auditor calls"      │
│ ✓ Repeatable methodology: "Not dependent on one person"         │
│ ✓ IP transfer: "Playbooks and frameworks transfer to you"       │
│ ✓ NIST alignment: "Enterprise-grade compliance posture"         │
├─────────────────────────────────────────────────────────────────┤
│ AVOID:                                                          │
│ ✗ Price competition (boutiques often undercut)                  │
│ ✗ Technical depth comparison (good boutiques are excellent)     │
│ ✗ Speed claims (small shops can be very fast)                   │
├─────────────────────────────────────────────────────────────────┤
│ KEY QUESTIONS TO ASK:                                           │
│ 1. "Ask them about governance documentation—what happens when   │
│     your auditor or regulator asks questions?"                  │
│ 2. "What happens if their key person leaves mid-project?"       │
│ 3. "What IP do you own after the engagement ends?"              │
│ 4. "How do they handle change management and adoption?"         │
├─────────────────────────────────────────────────────────────────┤
│ OBJECTION HANDLING:                                             │
│                                                                 │
│ "[Boutique] quoted 40% less"                                    │
│ → "That may be accurate on hourly rates. But what's the total   │
│    cost of a failed pilot? Team demoralization, board           │
│    credibility, opportunity cost, having to do it again?        │
│    Our methodology specifically addresses why 70-85% fail."     │
│                                                                 │
│ "They're more technical"                                        │
│ → "They may be—we're not trying to out-engineer data scientists.│
│    We're trying to ensure AI actually gets used and delivers    │
│    ROI. Technical excellence is necessary but not sufficient."  │
├─────────────────────────────────────────────────────────────────┤
│ PROOF POINTS:                                                   │
│ • NIST AI RMF alignment (boutiques rarely provide this)         │
│ • Failure mode countermeasures (documented methodology)         │
│ • INT Inc Starter Kit—reusable IP regardless of our involvement │
│ • 550+ case study patterns (not dependent on one person)        │
└─────────────────────────────────────────────────────────────────┘
```

### 4.3.4 Battle Card: vs. DIY

```
┌─────────────────────────────────────────────────────────────────┐
│                    INT INC vs. DIY                              │
│               (Internal build, hired specialists)               │
├─────────────────────────────────────────────────────────────────┤
│ LEAD WITH:                                                       │
│ ✓ Accelerated learning: "Skip 18-24 months of trial and error"  │
│ ✓ Risk reduction: "70-85% fail; our methodology addresses why"  │
│ ✓ Bandwidth: "Your team stays focused on core product"          │
│ ✓ Co-creation: "We work alongside you; you own everything"      │
├─────────────────────────────────────────────────────────────────┤
│ AVOID:                                                          │
│ ✗ "You can't do this"—insulting; they have smart people         │
│ ✗ Dependency framing—they fear consultant lock-in               │
│ ✗ Dismissing their expertise—they know their business better    │
├─────────────────────────────────────────────────────────────────┤
│ KEY QUESTIONS TO ASK:                                           │
│ 1. "What's your timeline for first measurable AI ROI?"          │
│ 2. "How much bandwidth does your best engineer have for AI      │
│     experimentation vs. core product?"                          │
│ 3. "What happens if your first 2-3 projects fail—what's the     │
│     impact on team morale and executive confidence?"            │
│ 4. "How will you build governance and compliance documentation?"│
├─────────────────────────────────────────────────────────────────┤
│ OBJECTION HANDLING:                                             │
│                                                                 │
│ "We have smart engineers"                                       │
│ → "Absolutely—and they should stay focused on your core         │
│    product. We're not replacing your team; we're accelerating   │
│    their AI learning. Work with us on pilot one, then your team │
│    leads pilots two through ten with our playbooks."            │
│                                                                 │
│ "We don't trust consultants"                                    │
│ → "I get it—consultants have a reputation for leaving behind    │
│    PowerPoints, not capability. Our model is different: we      │
│    transfer IP. The playbooks, templates, and frameworks we     │
│    build together are yours. We don't create dependency."       │
├─────────────────────────────────────────────────────────────────┤
│ PROOF POINTS:                                                   │
│ • 550+ case study patterns—you'd need 2 years to learn this     │
│ • INT Inc Starter Kit—you own all IP immediately                │
│ • Co-creation model—your team learns alongside ours             │
│ • 70-85% AI project failure rate—our methodology addresses this │
└─────────────────────────────────────────────────────────────────┘
```

---

## 4.4 Competitive Intelligence Operations

### 4.4.1 Ongoing Monitoring Protocol

**Weekly Tasks (30 min/week):**
- [ ] Review Google Alerts for: "AI consulting," "AI implementation," competitor names
- [ ] Check LinkedIn for competitor case study announcements
- [ ] Scan industry newsletters (AI Business, VentureBeat AI)

**Monthly Tasks (2 hrs/month):**
- [ ] Review competitor website changes (new services, case studies, pricing)
- [ ] Monitor job postings (reveals strategic priorities)
- [ ] Attend 1 competitor webinar (anonymously)
- [ ] Update competitive matrix with new intelligence

**Quarterly Tasks (4 hrs/quarter):**
- [ ] Analyze win/loss data; identify competitive patterns
- [ ] Update battle cards based on field feedback
- [ ] Review Gartner/Forrester reports on AI services market
- [ ] Refresh competitive positioning based on market shifts

### 4.4.2 Win/Loss Analysis Framework

**After Every Won Deal:**

| Question | Purpose |
|----------|---------|
| "Who else did you evaluate?" | Understand competitive set |
| "What made you choose us?" | Identify winning differentiators |
| "What almost made you choose someone else?" | Find weaknesses to address |
| "What could we have done better in the sales process?" | Improve process |

**After Every Lost Deal:**

| Question | Purpose |
|----------|---------|
| "Who did you select instead?" | Understand competitive loss |
| "What made them a better fit?" | Identify competitive gaps |
| "Was there anything we could have done differently?" | Find recoverable opportunities |
| "Would you consider us for future projects?" | Maintain relationship |

**Quarterly Aggregation:**

```
WIN/LOSS SUMMARY TEMPLATE

Period: Q[X] 2025

WINS: [count]
- Most common competitor displaced: [name]
- Top 3 winning factors:
  1. [factor] - [count]
  2. [factor] - [count]
  3. [factor] - [count]
  
LOSSES: [count]
- Most common winning competitor: [name]
- Top 3 loss factors:
  1. [factor] - [count]
  2. [factor] - [count]
  3. [factor] - [count]

ACTION ITEMS:
1. [action] - Owner: [name] - Due: [date]
2. [action] - Owner: [name] - Due: [date]
3. [action] - Owner: [name] - Due: [date]
```

### 4.4.3 Competitive Response Playbook

**When Competitor Is Already Engaged:**

*Scenario: Prospect says "We're already working with [competitor]"*

**Response Framework:**

```
Step 1: Acknowledge (don't disparage)
"That's great—how's it going?"

Step 2: Listen for pain signals
- Timeline delays
- Adoption challenges
- ROI uncertainty
- Governance gaps

Step 3: Offer complementary positioning
"We could potentially help accelerate [specific area they mentioned]
without disrupting what you've already started."

Step 4: Plant seeds for future
"If you're ever looking for a second perspective or need help 
with [adoption/governance/scaling], we'd be happy to chat."
```

**When Prospect Is Comparing Multiple Options:**

```
Step 1: Understand decision criteria
"What factors are most important in your decision?"

Step 2: Align to their criteria
"Based on what you've shared, here's how we compare on [criteria]..."

Step 3: Offer proof, not claims
"I can connect you with a client who was evaluating similar options..."

Step 4: Create urgency without pressure
"Our next pilot cohort starts [date]—if timing matters, let me know."
```

---

## 4.5 Competitive Pricing Intelligence

### 4.5.1 Known Competitor Pricing

| Competitor Type | Typical Engagement | Rate Range | Minimum |
|-----------------|-------------------|------------|---------|
| **Big 4** | Strategy + Assessment | $250-500/hr blended | $250K |
| **Big 4** | Implementation | $150-300/hr blended | $500K |
| **Accenture** | Platform implementation | $175-350/hr blended | $200K |
| **Technical boutique** | Custom ML solution | $150-250/hr | $50K |
| **Fractional AI leader** | Advisory | $15-30K/month | 3 months |
| **Implementation shop** | Fixed-scope build | $100-200/hr | $25K |

### 4.5.2 INT Inc Pricing Positioning

| Package | INT Inc Price | Competitor Equivalent | INT Inc Advantage |
|---------|--------------|----------------------|-------------------|
| Discovery | $8-15K | $50-100K (Big 4 assessment) | 5-7x less cost; 2-4 weeks vs. 3-6 months |
| Design | $15-25K | $100-250K (Big 4 architecture) | 5-10x less cost; senior delivery |
| Pilot | $15-50K | $200-500K (Big 4 pilot) | 5-10x less cost; faster results |
| SMB Quick Start | $8-15K | Not offered by Big 4 | Unique market segment |

### 4.5.3 Pricing Objection Responses

**"You're more expensive than [boutique]"**

> "We might be—by 20-30% on hourly rates. But here's what I'd ask: what's the total cost of a failed pilot? Team demoralization, executive credibility hit, opportunity cost of 6 months, having to hire someone else to do it again. Our methodology specifically addresses the failure modes that cause 70-85% of projects to miss ROI. The question isn't 'what's the hourly rate?'—it's 'what's the probability of success?'"

**"We can't afford $50K for a pilot"**

> "I understand budget constraints. Let me offer two alternatives: (1) Our SMB Quick Start at $8-15K, focused on one use case with a narrower scope. (2) A phased approach where we start with Discovery at $8-15K, validate the opportunity, and then you decide whether to continue. You're never committed to the full amount upfront."

**"Big 4 quoted similar for a much bigger scope"**

> "That may be true on paper. Ask about the team composition: what percentage is senior practitioners vs. junior analysts? Ask about timeline: when do you see first results? And ask about IP: what do you own when they leave? Our 'smaller' scope delivers measurable results in 6 weeks with senior delivery. Their 'bigger' scope might take 6 months to even finish assessment."

---

# TASK 5: SMB FAST TRACK PACKAGE (MAXIMUM DEPTH)

## 5.1 Product Definition

### 5.1.1 Package Overview

**Product Name:** INT Inc AI Quick Start for SMB

**Tagline:** "Enterprise AI methodology, right-sized for growth companies"

**Target Segment:** Organizations with 25-200 employees, $5M-50M revenue, B2B focus

**Investment:** $8,000-15,000 (Phase 1)

**Duration:** 6-10 weeks

**Scope:** 1 Quick Win initiative (support automation, document generation, or workflow optimization)

**Deliverables:** Working pilot + validated ROI + reusable IP transfer

### 5.1.2 Why SMB Needs a Different Package

| Factor | Enterprise Assumption | SMB Reality | Package Adaptation |
|--------|----------------------|-------------|-------------------|
| **Budget** | $100K+ for pilot | $10-25K total AI budget | Reduced scope; pre-built tools |
| **IT resources** | Dedicated team | Founder/ops wears IT hat | Turnkey setup; minimal config |
| **Executive time** | 10-15% sponsor allocation | Owner has 5 hats | Async updates; fewer meetings |
| **Data maturity** | Data warehouse, BI | Spreadsheets, HubSpot | Heavier Discovery; expectations reset |
| **Risk tolerance** | Governance committees | "Make it work" | Lighter governance framework |
| **Success timeline** | 18-24 month ROI | Need results in 6 months | Faster time-to-value |

### 5.1.3 Package Inclusions

**Phase 1: Quick Start ($8-15K, 6-10 weeks)**

| Week | Activities | Client Time | INT Inc Hours |
|------|------------|-------------|---------------|
| **1** | Kickoff call (90 min); simplified Discovery questionnaire; success criteria definition | 3 hrs | 8 hrs |
| **2** | Data readiness assessment; workflow mapping; architecture recommendation | 1 hr (async review) | 12 hrs |
| **3-4** | Build & configuration; integration setup; agent training materials | 2 hrs (testing) | 16 hrs |
| **5-6** | Go-live; monitoring setup; weekly tuning; adoption support | 2 hrs/week | 8 hrs |
| **7-8** | Results validation; ROI report; Phase 2 recommendations | 2 hrs | 8 hrs |
| **Total** | | ~12 hrs | ~52 hrs |

**Deliverables Included:**
- ✅ Working AI pilot (1 use case)
- ✅ Simplified governance framework (2-zone model)
- ✅ Agent/user training guide (5-page template)
- ✅ ROI validation report with actuals
- ✅ Phase 2 scaling recommendations
- ✅ Reusable IP (templates, prompts, workflows)

**Not Included (Phase 2 Add-Ons):**
- ❌ Custom integrations: +$3-5K each
- ❌ Additional channels: +$2-3K each
- ❌ Advanced analytics dashboard: +$2-4K
- ❌ Full NIST AI RMF documentation: +$3-5K
- ❌ On-site support: +$2-3K per visit

### 5.1.4 Use Case Selection Menu

| Use Case | Best Fit When | Expected ROI | Typical Timeline |
|----------|---------------|--------------|------------------|
| **AI Support Copilot** | 100+ tickets/month; existing KB | 2.5-3.5x Year 1 | 6-8 weeks |
| **Invoice Processing** | 100+ invoices/month; standard formats | 2.0-4.0x Year 1 | 8-10 weeks |
| **Tech Doc Generation** | 10+ docs/month; template-able | 1.5-2.5x Year 1 | 6-8 weeks |
| **Meeting Notes Automation** | 20+ meetings/week; action tracking | 1.5-2.0x Year 1 | 4-6 weeks |
| **Lead Qualification** | HubSpot/CRM; 50+ leads/month | 2.0-3.0x Year 1 | 6-8 weeks |

---

## 5.2 SMB ROI Framework

### 5.2.1 Adjusted Benchmarks

Enterprise benchmarks don't translate directly to SMB. Here are validated SMB adjustment factors:

| Metric | Enterprise Benchmark | SMB Multiplier | SMB Target | Rationale |
|--------|---------------------|----------------|------------|-----------|
| **Deflection rate** | 50-75% | ×0.65 | 33-49% | Smaller KB; less documentation |
| **Resolution time reduction** | 40-60% | ×0.80 | 32-48% | Similar tech; smaller scope |
| **Annual savings/employee** | $8,700-18,000 | ×0.65 | $5,700-11,700 | Lower base wages |
| **Adoption rate** | 89% (with sponsor) | ×0.95 | 85% | Flatter org; easier communication |
| **Time to positive ROI** | 12-18 months | ×0.75 | 9-14 months | Smaller scope; faster iteration |

### 5.2.2 SMB ROI Calculator Inputs

**For AI Support Copilot (most common SMB use case):**

```
INPUT VARIABLES:
─────────────────────────────────────────────────────────
Ticket volume (monthly):        _____ (min 100 for viable ROI)
Current cost per ticket:        $_____ (default: $18 for SMB)
Current first response time:    _____ hours
Current resolution time:        _____ hours
Support team size:              _____ FTEs
Average hourly wage (loaded):   $_____ (default: $35)
KB maturity score (1-5):        _____ (must be ≥3.0)

CALCULATED PROJECTIONS:
─────────────────────────────────────────────────────────
Target deflection rate:         35% (SMB-adjusted)
Monthly tickets deflected:      [volume × 0.35]
Monthly deflection savings:     [deflected × cost/ticket]
Annual deflection savings:      [monthly × 12]

Agent time savings:             30% reduction
Hours saved/agent/month:        [hours worked × 0.30]
Monthly efficiency savings:     [hours × hourly rate × team size]
Annual efficiency savings:      [monthly × 12]

TOTAL ANNUAL VALUE:             [deflection + efficiency]
INVESTMENT (Phase 1):           $12,000 (midpoint)
YEAR 1 ROI:                     [(Annual Value - Investment) / Investment]
PAYBACK PERIOD:                 [Investment / (Annual Value / 12)] months
```

### 5.2.3 Sample SMB ROI Scenario

**Client Profile:**
- B2B SaaS company, 45 employees
- 350 support tickets/month
- 2.5 FTE support team
- Current cost per ticket: $22
- HubSpot ticketing system
- KB score: 3.5 (adequate)

**Projections:**

| Metric | Calculation | Value |
|--------|-------------|-------|
| Target deflection | 350 × 35% | 123 tickets/month |
| Deflection savings | 123 × $22 × 12 | **$32,472/year** |
| Agent efficiency | 2.5 FTE × 173 hrs × 30% × $35 | **$4,537/month** |
| Annual efficiency | $4,537 × 12 | **$54,444/year** |
| **Total annual value** | | **$86,916/year** |
| Investment | Phase 1 | $12,000 |
| **Year 1 ROI** | ($86,916 - $12,000) / $12,000 | **624%** |
| **Payback period** | $12,000 / ($86,916 / 12) | **1.7 months** |

**Sensitivity Analysis:**

| Scenario | Deflection | Efficiency | Annual Value | ROI |
|----------|------------|------------|--------------|-----|
| Pessimistic (25% deflection, 20% efficiency) | $23,100 | $36,296 | $59,396 | 395% |
| Base case (35% deflection, 30% efficiency) | $32,472 | $54,444 | $86,916 | 624% |
| Optimistic (45% deflection, 35% efficiency) | $41,844 | $63,518 | $105,362 | 778% |

---

## 5.3 SMB Governance Framework (Simplified)

### 5.3.1 Two-Zone Model

Enterprise requires three zones (Green/Yellow/Red). For SMB, we simplify to two:

| Zone | AI Authority | Human Role | Examples |
|------|--------------|------------|----------|
| **Green** | AI handles autonomously | Spot-check audit (5%) | Password resets, order status, FAQ, scheduling |
| **Yellow** | AI drafts, human approves | Review before send | Account changes, refunds, complaints, technical issues |

**Note:** Red Zone (prohibited) is implicit—anything not explicitly Green or Yellow is human-only by default.

### 5.3.2 SMB Risk Assessment (4 Questions)

Replace the full Risk-Tiering Matrix with this simplified decision tree:

```
QUESTION 1: Could an AI error cost more than $500?
├── YES → Yellow Zone
└── NO → Continue to Q2

QUESTION 2: Could an AI error cause a customer to leave?
├── YES → Yellow Zone
└── NO → Continue to Q3

QUESTION 3: Is this topic regulated (financial, health, legal)?
├── YES → Yellow Zone
└── NO → Continue to Q4

QUESTION 4: Does this require a commitment or promise?
├── YES → Yellow Zone
└── NO → Green Zone (AI autonomous)
```

### 5.3.3 Minimum Viable Audit Trail

Enterprise requires comprehensive logging. For SMB, we require only:

| What to Log | Where | Retention |
|-------------|-------|-----------|
| AI suggestion made | Ticketing system | 90 days |
| Human action (accept/modify/reject) | Ticketing system | 90 days |
| Customer feedback/rating | Survey tool | 1 year |

**Implementation:** Most ticketing systems (HubSpot, Zendesk, Freshdesk) log this automatically. No custom development required.

---

## 5.4 SMB Engagement Model

### 5.4.1 Async-First Communication

| Enterprise Standard | SMB Adaptation | Rationale |
|--------------------|----------------|-----------|
| Weekly 60-min sponsor call | Bi-weekly 30-min check-in | Owner time is precious |
| Daily standup | Slack/email async updates | No bandwidth for daily calls |
| Formal change requests | Quick Slack approval | Move fast; low bureaucracy |
| 15-metric weekly scorecard | 5-metric simple dashboard | Fewer metrics to track |

### 5.4.2 Owner Time Commitment

**Total time over 8-week engagement: ~12-15 hours**

| Activity | Time | Format | When |
|----------|------|--------|------|
| Kickoff call | 90 min | Video | Week 1 |
| Discovery review | 30 min | Async (Loom video from INT Inc) | Week 1 |
| Design approval | 30 min | Video | Week 2 |
| Testing feedback | 30 min/week × 2 | Async | Weeks 4-5 |
| Weekly check-ins | 30 min × 3 | Video (optional) | Weeks 3, 5, 7 |
| Go/no-go decision | 30 min | Video | Week 8 |
| Results review | 30 min | Video | Week 8 |

### 5.4.3 Decision-Making Framework

**For SMBs, decisions should be fast. Our framework:**

| Decision Type | Who Decides | Timeline | Escalation |
|---------------|-------------|----------|------------|
| Scope changes <$2K | INT Inc PM | Same day | None |
| Scope changes $2-5K | Owner (async) | 48 hours | Call if urgent |
| Scope changes >$5K | Owner (call) | 1 week | Scheduled call |
| Technical choices | INT Inc | Same day | Inform owner |
| Go/no-go on launch | Owner | Week 8 call | N/A |

---

## 5.5 SMB Success Criteria

### 5.5.1 Adjusted Success Thresholds

| Metric | Enterprise Target | SMB Target | Rationale |
|--------|-------------------|------------|-----------|
| Deflection rate | ≥50% | ≥30% | Smaller KB, less documentation |
| CSAT improvement | ≥30% | ≥15% | SMB baseline often higher |
| First response time reduction | ≥50% | ≥30% | Similar tech capability |
| Quality score | ≥4.0 | ≥3.8 | Lighter QA process |
| Time to positive ROI | ≤18 months | ≤12 months | Smaller scope |

### 5.5.2 Go/No-Go Decision Tree

```
SMB PILOT GO/NO-GO (Week 8)

CHECK 1: Is CSAT maintained or improved?
├── NO (CSAT declined >5%) → HOLD: Diagnose root cause
└── YES → Continue

CHECK 2: Is deflection rate ≥25%?
├── NO → CONDITIONAL GO: Extend monitoring 4 weeks
└── YES → Continue

CHECK 3: Is quality score ≥3.5?
├── NO → HOLD: KB remediation required
└── YES → Continue

CHECK 4: Are there any safety incidents?
├── YES → HOLD: Governance review required
└── NO → ✅ GO: Scale to next phase
```

---

## 5.6 SMB Statement of Work Template

### SMB AI Quick Start — Statement of Work

**Client:** [Company Name]
**INT Inc Lead:** [Name]
**Effective Date:** [Date]
**Duration:** 8 weeks

---

#### 1. Scope

INT Inc will implement an AI-powered [Support Copilot / Invoice Processing / Document Generation] pilot for [Client], targeting [specific use case/ticket type/workflow].

**In Scope:**
- [Channel 1] support tickets for [topic category]
- Integration with [HubSpot / Zendesk / etc.]
- Green Zone automation for [specific ticket types]
- Yellow Zone drafting for [specific ticket types]

**Out of Scope:**
- [Channel 2/3] (Phase 2)
- [Complex ticket types requiring Red Zone handling]
- Custom integrations beyond standard connectors
- On-site support

---

#### 2. Timeline

| Week | Milestone | Deliverable |
|------|-----------|-------------|
| 1 | Kickoff + Discovery | Kickoff deck; completed questionnaire |
| 2 | Design | Architecture recommendation; zone mapping |
| 3-4 | Build | Configured AI system; integration complete |
| 5-6 | Pilot | Live in production; daily monitoring |
| 7-8 | Validate | Results report; Phase 2 recommendations |

---

#### 3. Success Criteria

| Metric | Baseline | Target | Measurement |
|--------|----------|--------|-------------|
| Deflection rate | [current %] | ≥30% | Ticketing system |
| CSAT | [current score] | Maintained or improved | Post-ticket survey |
| Quality score | N/A | ≥3.8 | INT Inc evaluation |
| First response time | [current hrs] | ≤[target hrs] | Ticketing system |

---

#### 4. Investment

**Phase 1 (this SOW):** $[8,000-15,000]

Payment Schedule:
- 50% at signing: $[amount]
- 50% at Week 8 results delivery: $[amount]

---

#### 5. Client Responsibilities

- [ ] Provide access to ticketing system (admin)
- [ ] Assign owner (2-3 hrs/week availability)
- [ ] Complete Discovery questionnaire (Week 1)
- [ ] Participate in testing (Weeks 5-6)
- [ ] Make go/no-go decision (Week 8)

---

#### 6. INT Inc Responsibilities

- [ ] Deliver all milestones per timeline
- [ ] Provide bi-weekly progress updates
- [ ] Ensure quality score ≥3.8 or pause for remediation
- [ ] Transfer all IP (templates, prompts, workflows) at conclusion

---

#### 7. Signatures

| Role | Name | Signature | Date |
|------|------|-----------|------|
| Client (Owner/Decision Maker) | | | |
| INT Inc (Engagement Lead) | | | |

---

## 5.7 SMB Case Study Template

### [Company Name]: From Manual Support to AI-Powered Efficiency

**Company Profile:**
- Industry: [B2B SaaS / Professional Services / etc.]
- Employees: [XX]
- Monthly ticket volume: [XXX]
- Support team: [X] FTEs

**Challenge:**
[2-3 sentences describing the pain point]

**Solution:**
INT Inc implemented an AI Support Copilot over 8 weeks, focusing on:
- [Specific use case 1]
- [Specific use case 2]

**Results:**

| Metric | Before | After | Impact |
|--------|--------|-------|--------|
| Tickets deflected | [X]% | [Y]% | +[Z]% |
| First response time | [X] hrs | [Y] hrs | -[Z]% |
| CSAT score | [X] | [Y] | +[Z]% |
| Support capacity | [X] tickets/agent | [Y] tickets/agent | +[Z]% |

**ROI:**
- Investment: $[XX,XXX]
- Annual savings: $[XX,XXX]
- Payback period: [X] months
- Year 1 ROI: [XXX]%

**Quote:**
> "[Testimonial from owner/sponsor]" — [Name], [Title], [Company]

---

# TASK 6: DOCUMENT DEPENDENCY MAP (MAXIMUM DEPTH)

## 6.1 Document Hierarchy Visualization

```
INT Inc. AI Consulting Documentation Architecture
══════════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────────────────────┐
│                    STRATEGIC LAYER                              │
│                    (Leadership / Board)                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────────────┐    ┌─────────────────────┐            │
│  │ INT Inc Complete    │───▶│ AI Master Reference │            │
│  │ AI Implementation   │    │ Guide v1.0          │            │
│  │ Blueprint           │    │ (consolidated source)│            │
│  └─────────┬───────────┘    └──────────┬──────────┘            │
│            │                           │                        │
│            ▼                           ▼                        │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │              Statistics Reference Sheet                  │   │
│  │              (single source of truth for metrics)        │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                    PLANNING LAYER                               │
│                    (Delivery / Operations)                      │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌──────────────────┐  ┌──────────────────┐  ┌──────────────┐  │
│  │ AI Support       │  │ Enhancement      │  │ Platform     │  │
│  │ Consulting       │  │ Package v2.0     │  │ Explorer     │  │
│  │ Package v0.2     │  │ (this doc series)│  │ v3.2 (HTML)  │  │
│  │ (00-09 files)    │  │                  │  │              │  │
│  └────────┬─────────┘  └────────┬─────────┘  └──────┬───────┘  │
│           │                     │                    │          │
│           └─────────────────────┼────────────────────┘          │
│                                 │                               │
└─────────────────────────────────┼───────────────────────────────┘
                                  │
                                  ▼
┌─────────────────────────────────────────────────────────────────┐
│                    EXECUTION LAYER                              │
│                    (Client-Facing Delivery)                     │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌───────────┐ │
│  │ Discovery   │ │ Risk        │ │ Pilot       │ │ Support   │ │
│  │ Question-   │ │ Tiering     │ │ Scorecard   │ │ Evaluation│ │
│  │ naire       │ │ Matrix      │ │             │ │ Scorecard │ │
│  │ (04_*.md)   │ │ (05_*.md)   │ │ (09_*.md)   │ │ (06_*.md) │ │
│  └─────────────┘ └─────────────┘ └─────────────┘ └───────────┘ │
│                                                                 │
│  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌───────────┐ │
│  │ ROI         │ │ NIST AI     │ │ 4-Agent     │ │ Sales     │ │
│  │ Calculator  │ │ RMF         │ │ Architecture│ │ Battle    │ │
│  │ Template    │ │ Checklist   │ │             │ │ Card      │ │
│  │ (03_*.md)   │ │ (08_*.md)   │ │ (07_*.md)   │ │ (NEW)     │ │
│  └─────────────┘ └─────────────┘ └─────────────┘ └───────────┘ │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## 6.2 Document Registry

### 6.2.1 Complete Document Inventory

| Doc ID | Document Name | Version | Type | Owner | Last Updated |
|--------|---------------|---------|------|-------|--------------|
| DOC-001 | INT Inc Complete AI Implementation Blueprint | 1.0 | Strategic | Kyle | Dec 2025 |
| DOC-002 | AI Master Reference Guide | 1.0 | Reference | Kyle | Dec 2025 |
| DOC-003 | Statistics Reference Sheet | 0.2 | Reference | Kyle | Dec 2025 |
| DOC-004 | AI Support Consulting Package | 0.2 | Methodology | Kyle | Dec 2025 |
| DOC-005 | ROI Calculator Template | 0.2 | Tool | Finance | Dec 2025 |
| DOC-006 | Discovery Questionnaire | 0.2 | Tool | Delivery | Dec 2025 |
| DOC-007 | Risk Tiering Matrix | 0.2 | Framework | Delivery | Dec 2025 |
| DOC-008 | Support Evaluation Scorecard | 0.2 | Tool | Quality | Dec 2025 |
| DOC-009 | 4-Agent Architecture | 0.2 | Technical | Engineering | Dec 2025 |
| DOC-010 | NIST AI RMF Checklist | 0.2 | Compliance | Legal | Dec 2025 |
| DOC-011 | Pilot Scorecard | 0.2 | Tool | Delivery | Dec 2025 |
| DOC-012 | Platform Explorer | 3.2 | Tool (HTML) | Kyle | Dec 2025 |
| DOC-013 | Enhancement Package v2 | 2.0 | Strategic | Kyle | Dec 2025 |
| DOC-014 | Phase 2 Build-Out | 1.0 | Strategic | Kyle | Dec 2025 |

### 6.2.2 File Location Map

```
/INT_Inc_Documentation/
│
├── /Strategic/
│   ├── INT_Inc_Complete_AI_Implementation_Blueprint.md    [DOC-001]
│   ├── INTINC_AI_Master_Reference_v1.md                   [DOC-002]
│   └── INT_Inc_Blueprint_Enhancement_Package_v2.md        [DOC-013]
│
├── /Reference/
│   └── 01_statistics_reference_sheet.md                   [DOC-003]
│
├── /Methodology/
│   └── 02_INTINC_AI_Support_Consulting_v02.md            [DOC-004]
│
├── /Tools/
│   ├── 03_ROI_Calculator_Template.md                      [DOC-005]
│   ├── 04_Discovery_Questionnaire.md                      [DOC-006]
│   ├── 06_Support_Evaluation_Scorecard.md                 [DOC-008]
│   ├── 09_Pilot_Scorecard.md                              [DOC-011]
│   └── INT_Platform_Explorer_v3.2.html                    [DOC-012]
│
├── /Frameworks/
│   ├── 05_Risk_Tiering_Matrix.md                          [DOC-007]
│   ├── 07_4_Agent_Architecture.md                         [DOC-009]
│   └── 08_NIST_AI_RMF_Checklist.md                        [DOC-010]
│
└── /Phase2/
    └── INT_Inc_Phase2_Complete_Build_v1.md                [DOC-014]
```

---

## 6.3 Dependency Matrix

### 6.3.1 What Changes When You Change...

| If You Change... | Also Update... | Why | Priority |
|------------------|----------------|-----|----------|
| **Statistics Reference Sheet** | ROI Calculator, Platform Explorer, All proposals, Master Reference | All metrics flow from single source | 🔴 CRITICAL |
| **ROI Calculator Template** | Enhancement Package A (ROI Methodology), SMB Framework | ROI formulas must align | 🔴 CRITICAL |
| **4-Agent Architecture** | Consulting Package §2.3, Master Reference §9.4, Design deliverables | Technical foundation | 🟡 HIGH |
| **Risk Tiering Matrix** | NIST Checklist, Governance Framework, Pilot Scorecard | Zone definitions | 🟡 HIGH |
| **Success Criteria Templates** | Pilot Scorecard, Discovery Questionnaire, SOW templates | Metrics must align | 🟡 HIGH |
| **Platform Pricing** | Platform Explorer, ROI Calculator | Financial calculations | 🟡 HIGH |
| **Case Study Data** | Master Reference §7, Sales materials, Proposals | Proof points | 🟢 MEDIUM |
| **Competitive Intelligence** | Battle Cards, Sales playbook, Proposals | Market positioning | 🟢 MEDIUM |
| **Character Encoding** | All .md files | Display consistency | 🟢 LOW |

### 6.3.2 Cross-Reference Index

**Where Key Concepts Are Defined:**

| Concept | Primary Definition | Also Referenced In |
|---------|-------------------|-------------------|
| **4-Agent Pattern** | 02_*.md §2.3 | Master Reference §9.4, 07_*.md, Blueprint |
| **Deflection Rate** | 09_*.md §3.1 | ROI Calculator, Statistics Sheet, Enhancement Pkg |
| **Green/Yellow/Red Zones** | 05_*.md (Risk Matrix) | NIST Checklist, Consulting Pkg, Governance Framework |
| **KB Quality Score** | 04_*.md §3 (Discovery) | Failure Countermeasures (Enhancement Pkg) |
| **ROI per $1 Invested** | 01_*.md §2 (Statistics) | ROI Calculator, Master Reference §5.1, Enhancement Pkg |
| **WSJF Scoring** | Blueprint Component 3 | Initiative Portfolio, Master Reference |
| **1.7x Average ROI** | 01_*.md §2 (Statistics) | Master Reference, ROI Methodology |
| **$3.50 Customer Service ROI** | 01_*.md §2 (Statistics) | Sales materials, Proposals, Battle Card |
| **89% vs 34% Adoption** | 01_*.md (Statistics) | Failure Countermeasures, Sales materials |

### 6.3.3 Version Synchronization Rules

**Semantic Versioning:**
- **Major (X.0)**: Breaking changes; new methodology or framework
- **Minor (0.X)**: New content; non-breaking additions
- **Patch (0.0.X)**: Typos, formatting, encoding fixes

**Synchronization Groups:**

| Group | Documents | Rule |
|-------|-----------|------|
| **AI Support Package** | 00-09_*.md files | Must share same minor version; update together |
| **Master Reference** | INTINC_AI_Master_Reference_v1.md | Update when any source document changes |
| **Platform Explorer** | INT_Platform_Explorer_v3.X.html | Independent; monthly pricing refresh |
| **Blueprint** | INT_Inc_Complete_AI_Implementation_Blueprint | Update when strategy/methodology changes |
| **Enhancement Package** | INT_Inc_Blueprint_Enhancement_Package_vX.md | Independent; supplements Blueprint |

---

## 6.4 Change Management Protocol

### 6.4.1 Document Update Checklist

```
BEFORE UPDATING ANY DOCUMENT:

☐ Check dependency matrix for downstream impacts
☐ Identify all documents that reference changed content
☐ Create update list with specific sections to modify
☐ If metrics change: update Statistics Reference Sheet FIRST

DURING UPDATE:

☐ Update version number per semantic versioning
☐ Update "Last Updated" date
☐ Add entry to Document Control section
☐ Ensure consistent terminology across documents

AFTER UPDATE:

☐ Update all dependent documents per dependency matrix
☐ Run character encoding check (no UTF-8 artifacts)
☐ Verify cross-references are still accurate
☐ Notify stakeholders of changes

FOR PLATFORM EXPLORER (HTML):

☐ Validate all JSON data structures
☐ Test all filters, sorts, exports
☐ Verify pricing is current
☐ Check that statistics match Statistics Reference Sheet
```

### 6.4.2 Quarterly Review Cycle

| Task | Frequency | Owner | Timing |
|------|-----------|-------|--------|
| Statistics validation | Quarterly | Kyle | Week 1 of quarter |
| Platform pricing refresh | Monthly | Kyle | 1st of month |
| Competitive intelligence update | Quarterly | Sales | Week 2 of quarter |
| Case study refresh | Quarterly | Delivery | Week 3 of quarter |
| Full document audit | Semi-annual | Kyle + Ops | Jan, Jul |

### 6.4.3 Single Source of Truth Enforcement

**Rule:** When the same information appears in multiple documents, ONE document is the authoritative source. All others must reference that source.

| Information Type | Authoritative Source | Update Process |
|------------------|---------------------|----------------|
| Industry statistics | Statistics Reference Sheet | Update sheet first; propagate to all |
| ROI formulas | ROI Calculator Template | Update calculator; then methodology docs |
| Technical architecture | 07_4_Agent_Architecture.md | Update architecture; then references |
| Governance framework | 05_Risk_Tiering_Matrix.md | Update matrix; then NIST, consulting pkg |
| Platform data | Platform Explorer HTML | Update HTML; export feeds other materials |
| Competitive positioning | Enhancement Package (Component D) | Update competitive section; then battle cards |

---

# TASK 7: PLATFORM EXPLORER SALES INTEGRATION (MAXIMUM DEPTH)

## 7.1 Sales Process Integration Map

### 7.1.1 Where Platform Explorer Fits in Sales Cycle

| Sales Stage | Platform Explorer Usage | Purpose |
|-------------|------------------------|---------|
| **Cold Outreach** | Not used | Too detailed for first touch |
| **Discovery Call** | Statistics tab (1-2 min) | Establish credibility with market data |
| **Qualification** | ROI Calculator tab (5-10 min) | Model their specific opportunity |
| **Solution Presentation** | Platform Comparison tab (5-10 min) | Show technology options |
| **Technical Validation** | Feature Matrix tab (10-15 min) | Deep-dive for technical buyers |
| **Proposal** | Export (Markdown/PDF) | Include comparison as appendix |
| **Negotiation** | ROI Calculator (revisit) | Justify investment with validated numbers |

### 7.1.2 Tab-by-Tab Usage Guide

**Statistics Dashboard Tab**

*When to use:* First 2 minutes of any call where credibility needs establishing

*Talk track:*
> "Let me share some context on the AI landscape. [Open Statistics tab] You'll see 88% of organizations are now using AI—but only 6% are high performers. That gap represents our opportunity. And look here: 70-85% of AI projects fail to meet ROI expectations. That's not a technology problem; it's a methodology problem."

*Key stats to highlight:*
- 88% adoption / 6% high performers (the gap)
- 70-85% failure rate (the problem we solve)
- $3.50 per $1 in customer service (the opportunity)
- 89% vs 34% adoption (the methodology difference)

---

**Platform Comparison Tab**

*When to use:* When prospect asks "what AI tools should we use?" or mentions specific platforms

*Talk track:*
> "Great question. Let me show you how the major platforms compare. [Open Comparison tab, select 2-3 relevant platforms] See this radar chart? It shows how each platform scores across our five key dimensions. [Point to specific strengths/weaknesses] Based on what you've told me about your priorities, here's what I'd recommend exploring..."

*Demo flow:*
1. Start with "Select platforms to compare"
2. Choose 2-3 platforms relevant to their situation
3. Show radar chart for visual differentiation
4. Scroll to feature matrix for specifics
5. End with recommendation

*Key comparisons by situation:*
- "We use Microsoft 365" → Compare: M365 Copilot, ChatGPT Enterprise, Claude
- "We're on Salesforce" → Compare: Einstein, Agentforce, Claude
- "We want best-in-class AI" → Compare: Claude, GPT-4, Gemini
- "Security is paramount" → Compare: Claude, AWS Bedrock, Azure OpenAI

---

**ROI Calculator Tab**

*When to use:* After understanding their situation; to quantify the opportunity

*Talk track:*
> "Let me put your numbers in and see what the model shows. [Open ROI Calculator] What's your monthly ticket volume? [Enter their numbers] And your current cost per ticket—often it's around $20, but what's your estimate? [Continue entering] 
>
> [After calculations display]
>
> See this? That's [$ amount] in projected annual savings from deflection alone. Now add efficiency gains—your agents saving [X] hours per week... That's another [$ amount]. Total projected value: [$ amount] on an investment of [$ amount]. That's a [X]-month payback.
>
> Now, these are projections. Our methodology validates these numbers week by week during the pilot. If we're not tracking, we adjust or pivot. You're never surprised at Week 6."

*Key inputs to gather before demo:*
- Monthly ticket volume
- Support team size
- Average cost per ticket (or use $20 default)
- Current first response time
- Current resolution time

---

**Feature Matrix Tab**

*When to use:* With technical buyers who want detailed comparison

*Talk track:*
> "Let me show you the detailed feature comparison. [Open Feature Matrix] Here you can see exactly how each platform handles [specific requirement they mentioned]. Notice that [Platform A] scores higher on [feature], but [Platform B] has better [other feature]. Given your requirements around [their priority], I'd focus on [recommendation]."

*Pro tip:* Don't show this tab to executives—too detailed. Save for technical validation with IT/engineering.

---

**Export Functions**

*When to use:* After demo, to provide leave-behind; in proposal appendix

*Available formats:*
| Format | Use Case | When to Use |
|--------|----------|-------------|
| PDF | Executive summary | Proposal appendix; board presentation |
| Markdown | GitHub/technical docs | Technical teams who prefer text |
| CSV | Data analysis | When they want to do their own analysis |
| JSON | Integration | If they want to import data |

*Talk track when offering export:*
> "I can send you this comparison in whatever format is most useful. PDF works well for sharing with your team, or I can send the raw data if you want to do your own analysis."

---

## 7.2 Demo Scripts

### 7.2.1 Script: First Discovery Call (5 minutes)

**Setup:** Open Platform Explorer before call; have Statistics tab ready

**Minute 1-2: Credibility**
> "Before we dive into your situation, let me share some context on what we're seeing in the market. [Show Statistics tab]
>
> 88% of organizations are now experimenting with AI. But here's what's interesting—only 6% are achieving enterprise-wide impact. That's an 82-point gap between experimentation and value.
>
> And this stat really matters: 70-85% of AI projects fail to meet ROI expectations. Not because the technology doesn't work—but because the implementation approach is flawed."

**Minute 2-3: Transition to Their Situation**
> "That's why methodology matters more than technology. Speaking of which—tell me about your AI initiatives. What have you tried so far?"

**[Listen; take notes]**

**Minute 4-5: Tease Value**
> "Based on what you're describing, let me show you something. [Switch to ROI Calculator tab] If your volume is roughly [X] tickets per month, even a conservative deflection rate would mean... [run quick calculation]
>
> That's worth exploring further. Can we schedule 30 minutes next week to dig deeper into your specific situation?"

---

### 7.2.2 Script: ROI Deep-Dive (15 minutes)

**Setup:** Have their data from discovery (ticket volume, team size, etc.)

**Minute 1-2: Recap**
> "Last time we talked about the 6% vs 94% gap. Today I want to make this concrete for your situation. I'm going to put your numbers in our model and show you what's possible."

**Minute 3-8: Live Calculation**
> "Let me input what you shared: [Enter their data while narrating]
>
> - Monthly tickets: [X]
> - Team size: [X] FTEs
> - Cost per ticket: $[X]
> - Current deflection: [X]% (if any)
>
> [As results appear]
>
> Here's what the model shows:
> - Target deflection of 35% would deflect [X] tickets per month
> - That's $[X] in annual deflection savings
> - Plus your agents recover [X] hours each—that's another $[X]
> - Total annual value: $[X]
> - On an investment of $[X], that's a [X]-month payback
>
> Now let me show you the sensitivity analysis... [Show range]
>
> Even in our pessimistic scenario—25% deflection, 20% efficiency—you're still looking at $[X] in annual value. The question isn't 'does this work?'—it's 'how do we implement it without being part of the 70% that fail?'"

**Minute 9-12: Platform Discussion**
> "One thing that makes us different: we're technology-agnostic. Let me show you how the platforms compare. [Switch to Comparison tab]
>
> Based on your environment—you mentioned you're on [Microsoft/Salesforce/etc.]—here's what I'd recommend considering...
>
> [Show relevant comparison]
>
> We'll validate the right platform during Design, but the technology is maybe 30% of success. The 70% is what we talked about: data readiness, workflows, change management, governance."

**Minute 13-15: Next Steps**
> "Based on what we've discussed, I think there's a strong case here. The question is how to move forward without risking a failed pilot.
>
> Our approach is a 6-week pilot on one use case. We validate these projections in the real world before you commit to anything larger.
>
> Can I send over a scope document for your review? If the numbers work, we could kick off in [timeframe]."

---

### 7.2.3 Script: Technical Deep-Dive (20 minutes)

**Setup:** Technical audience (IT, engineering); they've seen high-level pitch

**Minute 1-3: Set Context**
> "I know you've seen the business case. Let me walk you through the technical architecture and how we evaluate platforms. [Open Platform Explorer to Feature Matrix]
>
> First question: what are your must-have requirements? [Note them]
>
> Let me show you how the platforms we're considering compare on those specific points..."

**Minute 4-10: Feature Comparison**
> "[Show Feature Matrix filtered to relevant platforms]
>
> On [Requirement 1], you'll see [Platform A] scores [X] because [reason]. [Platform B] handles it differently—[explanation].
>
> For your compliance requirements, here's the comparison... [Show compliance row]
>
> API access and customization—this matters for integration with your stack... [Show relevant rows]"

**Minute 11-15: Architecture Discussion**
> "Let me show you how we implement this. [Reference 4-Agent Architecture]
>
> Our pattern uses four specialized agents rather than a single chatbot:
> 1. Classifier: Triage and routing
> 2. Retriever: Knowledge gathering
> 3. Drafter: Response generation
> 4. Validator: Quality and compliance
>
> This architecture means no single agent produces customer-facing output without validation. It's enterprise-grade because [technical benefits]."

**Minute 16-18: Integration Points**
> "For integration with your current stack:
> - Ticketing system: We use [standard connectors]
> - Knowledge base: We connect to [their KB]
> - CRM: [If relevant]
>
> Most implementations require minimal custom development because we use pre-built connectors."

**Minute 19-20: Technical Q&A**
> "What questions do you have about the technical approach?"

---

## 7.3 Demo Environment Setup

### 7.3.1 Pre-Call Checklist

```
PLATFORM EXPLORER DEMO PREP
───────────────────────────────────────────────────────────────

☐ Open Platform Explorer in browser (Chrome recommended)
☐ Test all tabs load correctly
☐ Pre-select any platforms relevant to this prospect
☐ Have ROI Calculator ready with default values
☐ Test export functions work

PROSPECT RESEARCH
───────────────────────────────────────────────────────────────

☐ Note their current tech stack (Microsoft? Salesforce? Other?)
☐ Estimate their ticket volume (LinkedIn, job postings, etc.)
☐ Identify likely pain points
☐ Check for any AI announcements or initiatives
☐ Note competitor relationships

TALKING POINTS PREPARED
───────────────────────────────────────────────────────────────

☐ 3 statistics relevant to their industry
☐ 2-3 platform comparisons relevant to their stack
☐ Rough ROI scenario with estimated inputs
☐ 2-3 proof points / case studies relevant to their size/industry
```

### 7.3.2 Common Demo Issues and Fixes

| Issue | Cause | Fix |
|-------|-------|-----|
| Radar chart doesn't render | SVG not supported in browser | Use Chrome; avoid IE/old Edge |
| Export fails | Pop-up blocker | Disable for this site |
| Numbers seem off | Stale statistics | Refresh Platform Explorer; clear cache |
| Filter doesn't work | JavaScript error | Hard refresh (Ctrl+Shift+R) |
| Slow loading | Large file | Pre-load before call; close other tabs |

---

## 7.4 Post-Demo Follow-Up

### 7.4.1 Follow-Up Email Template

```
Subject: INT Inc Platform Comparison + ROI Analysis - [Company Name]

Hi [Name],

Thank you for the conversation today. As promised, here's the comparison 
and ROI analysis we discussed.

KEY FINDINGS:
• Your estimated annual value: $[X] (based on [Y] monthly tickets)
• Payback period: [X] months on a $[X] pilot investment
• Recommended platform approach: [Brief recommendation]

ATTACHMENTS:
• Platform Comparison (PDF) - summarizes our discussion on [platforms]
• ROI Scenario (PDF) - your specific numbers

NEXT STEPS:
1. Review the attached materials
2. Let me know if you have questions
3. If the numbers work, I can send over a scope document

The key question isn't whether AI can help—it's how to implement it 
without being part of the 70% that fail. That's where our methodology 
comes in.

Looking forward to your thoughts.

Best,
[Name]
```

### 7.4.2 Attachment Checklist

| Attachment | When to Include | Export Format |
|------------|-----------------|---------------|
| Platform Comparison | Always | PDF |
| ROI Scenario | After ROI discussion | PDF (from calculator) |
| Feature Matrix | For technical buyers | PDF or CSV |
| Case Study | If relevant industry/size | PDF |
| Statistics Summary | If they asked about market data | PDF |

---

## 7.5 Platform Explorer Maintenance for Sales

### 7.5.1 Monthly Update Checklist

```
PLATFORM EXPLORER SALES MAINTENANCE (Monthly)
─────────────────────────────────────────────────────────────

WEEK 1: PRICING VERIFICATION
☐ Check all platform pricing against vendor websites
☐ Update any changed pricing in PLATFORMS_DATA
☐ Verify pricing notes are current

WEEK 2: STATISTICS REFRESH
☐ Check Statistics Reference Sheet for updates
☐ Update STATISTICS object if metrics changed
☐ Verify all displayed stats match source

WEEK 3: COMPETITIVE INTELLIGENCE
☐ Check for new platform features announced
☐ Update platform strengths/weaknesses if needed
☐ Add any new platforms to consideration list

WEEK 4: FIELD FEEDBACK
☐ Collect feedback from sales team on demo effectiveness
☐ Note any common questions not addressed
☐ Identify requested enhancements

VERSION UPDATE (if changes made):
☐ Increment version number (3.2 → 3.3)
☐ Update timestamp in footer
☐ Test all functionality
☐ Distribute updated version
```

---

# INTEGRATION & DEPLOYMENT

## Integration Index

| Phase 2 Component | Integrates With | How |
|-------------------|-----------------|-----|
| Competitive Intelligence (4) | Sales playbook, Battle Cards, Proposals | Direct usage; copy competitive language |
| SMB Fast Track (5) | ROI Calculator, Consulting Package | Variant pricing; adjusted methodology |
| Document Dependency Map (6) | All documents | Reference for updates; version control |
| Platform Explorer Integration (7) | Sales process, Demo workflow | Training material; scripts |

## Deployment Checklist

### Competitive Intelligence
- [ ] Distribute Battle Cards to sales team
- [ ] Conduct 30-min competitive training session
- [ ] Add competitive questions to CRM/discovery template
- [ ] Schedule quarterly win/loss review

### SMB Fast Track
- [ ] Update pricing page/materials with SMB package
- [ ] Create SMB-specific landing page content
- [ ] Train sales on SMB qualification criteria
- [ ] Set up SMB SOW template in document system

### Document Dependency Map
- [ ] Share map with all content owners
- [ ] Set up quarterly review calendar
- [ ] Create update notification process
- [ ] Establish document approval workflow

### Platform Explorer Sales Integration
- [ ] Conduct sales team demo training (60 min)
- [ ] Distribute demo scripts
- [ ] Set up pre-call checklist in CRM
- [ ] Schedule monthly Platform Explorer maintenance

---

# VERSION CONTROL

## Document History

| Version | Date | Changes |
|---------|------|---------|
| 1.0 | December 11, 2025 | Initial Phase 2 maximum depth build-out |

## Approval

| Role | Name | Date | Signature |
|------|------|------|-----------|
| Sales Lead | | | |
| Delivery Lead | | | |
| Marketing Lead | | | |

---

*INT Inc. Phase 2: Complete Sales & Market Enablement Build-Out*
*Version 1.0 | December 11, 2025*
*All 4 Phase 2 Tasks — Maximum Depth with Full Subphases*
